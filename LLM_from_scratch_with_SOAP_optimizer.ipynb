{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-optimizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WfCCXg31xMq",
        "outputId": "72acd8a8-fd4b-43f4-b948-b3d3b2508056"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-optimizer\n",
            "  Downloading pytorch_optimizer-3.2.0-py3-none-any.whl.metadata (59 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/59.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>1.24.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-optimizer) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.10/dist-packages (from pytorch-optimizer) (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->pytorch-optimizer) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->pytorch-optimizer) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->pytorch-optimizer) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->pytorch-optimizer) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->pytorch-optimizer) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->pytorch-optimizer) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10->pytorch-optimizer) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10->pytorch-optimizer) (3.0.2)\n",
            "Downloading pytorch_optimizer-3.2.0-py3-none-any.whl (192 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.8/192.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytorch-optimizer\n",
            "Successfully installed pytorch-optimizer-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0POAl43zkJbB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "def create_1d_rope_embeddings(seq_len, dim, device='cpu'):\n",
        "    # Ensure the dimension is even\n",
        "    assert dim % 2 == 0, \"Dimension must be even for 1D RoPE\"\n",
        "\n",
        "    # Create position indices\n",
        "    inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
        "    inv_freq = inv_freq.to(device)\n",
        "\n",
        "    # Create position embeddings\n",
        "    sinusoid_inp = torch.einsum(\"i,j->ij\", torch.arange(seq_len, dtype=torch.float, device=device), inv_freq)\n",
        "\n",
        "    # Create the RoPE embeddings\n",
        "    rope_sin = torch.sin(sinusoid_inp).unsqueeze(1).permute(1, 0, 2)\n",
        "    rope_cos = torch.cos(sinusoid_inp).unsqueeze(1).permute(1, 0, 2)\n",
        "\n",
        "    return rope_sin, rope_cos\n",
        "\n",
        "def apply_1d_rope(token_embeddings, rope_sin, rope_cos):\n",
        "    # Ensure the token embeddings dimension is even\n",
        "    assert token_embeddings.shape[-1] % 2 == 0, \"Token embeddings dimension must be even for 1D RoPE\"\n",
        "\n",
        "    # Split the token embeddings into even and odd dimensions\n",
        "    token_embeddings_even = token_embeddings[..., 0::2]\n",
        "    token_embeddings_odd = token_embeddings[..., 1::2]\n",
        "\n",
        "    # Apply the RoPE embeddings\n",
        "    token_embeddings_even_rotated = token_embeddings_even * rope_cos - token_embeddings_odd * rope_sin\n",
        "    token_embeddings_odd_rotated = token_embeddings_even * rope_sin + token_embeddings_odd * rope_cos\n",
        "\n",
        "    # Concatenate the rotated embeddings\n",
        "    token_embeddings_rotated = torch.cat((token_embeddings_even_rotated, token_embeddings_odd_rotated), dim=-1)\n",
        "    return token_embeddings_rotated"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For demonstration, assume we have a list of texts\n",
        "texts = [\n",
        "    \"Lorem ipsum odor amet, consectetuer adipiscing elit. Sem dis justo dapibus hendrerit porta. Consequat tincidunt adipiscing dapibus, montes eu nunc curabitur. Purus rutrum sit venenatis est aliquet semper litora vehicula. Porta porttitor ligula tristique ad risus nec vitae. Malesuada libero volutpat pretium at semper potenti purus pretium hendrerit. Fermentum arcu facilisi dapibus tellus nibh dui, hac placerat. Egestas eros suspendisse semper nibh porta leo amet.\",\n",
        "    \"Molestie et fusce penatibus ad nisi eros. Maecenas habitant hendrerit praesent aptent convallis. Quam adipiscing feugiat conubia posuere dui dis. Tempus dapibus aliquet facilisi taciti maximus. Imperdiet lorem dolor est; hac dapibus vehicula. Class turpis varius natoque vitae nam vehicula sociosqu pellentesque. Semper habitant dictumst magnis inceptos odio elementum platea. Vitae lacinia donec praesent est, curabitur viverra etiam cursus. Nisl platea mauris dapibus mauris pellentesque, nisl lobortis elit.\",\n",
        "    \"Aliquet ut mus cubilia ullamcorper ad ut tellus. Eros vel metus aliquam aenean suscipit ante eget? Ad lacus fermentum mus tempor congue imperdiet blandit himenaeos. Maecenas turpis penatibus consequat bibendum tincidunt non nibh. Placerat urna consequat; finibus eleifend hendrerit rhoncus velit id iaculis. Aliquet est ultricies donec interdum tellus scelerisque facilisis. Rhoncus senectus in quis finibus taciti; ornare dis. Habitasse imperdiet justo tristique massa fringilla natoque, odio sit. Dui nullam facilisis fusce sapien hendrerit sapien dis molestie.\"\n",
        "    \"Orci vulputate vestibulum taciti finibus non interdum. Amet lorem integer fermentum rutrum ornare orci. Cras mollis dapibus lobortis; vehicula pharetra duis auctor hendrerit cursus. Justo ultricies egestas id arcu enim posuere interdum. Condimentum eleifend dictum nec penatibus montes ridiculus tellus lacinia. Conubia montes duis neque enim nisl conubia. Tempus neque ultrices vel adipiscing gravida praesent aptent. Scelerisque sapien sociosqu suspendisse tortor habitasse eleifend. Vulputate conubia lectus augue, senectus maximus fermentum habitant. Nascetur hac cras eros donec magna feugiat quam netus malesuada.\"\n",
        "    \"Inceptos quisque placerat himenaeos facilisi primis. Nostra natoque sem penatibus lectus dolor. Arcu consequat aliquam sodales taciti aptent. Mattis nisi mauris himenaeos blandit lacus et? Odio elementum sagittis eu nisi aliquam maecenas vulputate. Non convallis himenaeos est nostra congue dignissim tempus feugiat. Enim integer tellus mattis velit fermentum massa. Tristique ad aliquet blandit elit ante dis varius viverra.\"\n",
        "    \"Iaculis nulla sagittis id ridiculus ad vestibulum; ante augue adipiscing? Tempus hendrerit orci curabitur, velit dapibus vulputate tincidunt. Faucibus varius scelerisque malesuada at quisque tristique urna. Nisi commodo consequat aenean facilisis viverra suspendisse lacinia finibus. Lorem torquent adipiscing tincidunt sed aenean. Maecenas finibus posuere eu parturient dapibus vehicula varius. Felis justo facilisis inceptos ex etiam turpis, risus sodales. Elit vel tempus habitant sodales arcu eros taciti at neque.\",\n",
        "    \"Vulputate dis interdum tincidunt ultricies molestie sociosqu sapien. Porta eu porta fusce nam hendrerit mollis elit. Dignissim mauris est class eu integer. Litora sociosqu blandit diam feugiat, dictum mus taciti eget. Condimentum habitant nostra ultricies facilisis habitasse. Montes dapibus maximus sapien penatibus inceptos mollis. Nostra cursus conubia ad suscipit tellus sociosqu volutpat. Nam vitae varius montes curae ante lectus diam.\",\n",
        "    \"Fermentum convallis potenti nam turpis gravida penatibus. Integer venenatis rutrum pretium donec hac. Lectus at malesuada velit curae vehicula netus vestibulum. Libero tincidunt id varius potenti diam accumsan. Eros litora urna fames, interdum luctus erat. Auctor fringilla malesuada commodo sociosqu maecenas. Risus convallis molestie posuere vel ante tristique facilisis ligula. Pulvinar parturient turpis mollis suspendisse ante blandit.\",\n",
        "    \"Efficitur aptent ac viverra ullamcorper eget sollicitudin eget cursus. Erat bibendum penatibus praesent quam sollicitudin. Ante quam faucibus mus volutpat phasellus penatibus. Senectus scelerisque turpis mus feugiat viverra morbi sociosqu. Class nec placerat ex, a hac nec. Curabitur quis fermentum tincidunt neque augue lacinia. Aex conubia ex vivamus ligula proin sit. Bibendum habitasse cursus congue morbi platea. Taciti est finibus accumsan iaculis montes rutrum. Consequat vulputate odio conubia sociosqu imperdiet porta!\",\n",
        "    \"Dictumst quisque tellus dui tellus velit purus eget venenatis. Nec consequat aliquet curae tincidunt quam nisi. Auctor placerat class; sem phasellus eu netus. Non sodales suscipit molestie, egestas dapibus hac. Natoque feugiat imperdiet venenatis parturient hendrerit scelerisque finibus. Lacinia cubilia turpis libero mus velit dui tellus parturient nulla. Aenean dapibus aliquet penatibus himenaeos elit turpis. Ridiculus pretium facilisis vulputate condimentum cursus sed. Taciti urna gravida pellentesque nostra elementum, nisi efficitur torquent rutrum. Facilisis mauris varius sollicitudin accumsan turpis efficitur efficitur aliquam orci.\",\n",
        "    \"Ligula adipiscing phasellus condimentum malesuada iaculis sed laoreet eros auctor. Conubia libero dignissim feugiat at lorem curae elementum convallis. Curabitur leo habitasse posuere quisque tortor vehicula nibh ac. Adipiscing odio purus nullam nisl; fusce dapibus. Elit maximus laoreet libero mus consectetur purus augue suspendisse. Neque lobortis diam quam mus; natoque accumsan. Urna adipiscing sit consequat posuere orci rhoncus quis sem. Consequat proin curae eleifend mauris dignissim magna laoreet.\",\n",
        "    \"Himenaeos euismod malesuada quisque diam nam tincidunt. Justo nam lacinia vulputate elit justo class mi augue. Mollis dignissim imperdiet himenaeos, pellentesque sit primis? Sed posuere dui amet nunc est. Euismod praesent ornare eros vivamus vel habitant leo. Natoque rutrum suscipit pulvinar torquent lacinia iaculis pretium eget? Accumsan praesent elit euismod orci pharetra interdum quisque ad. Convallis vitae rutrum blandit, non justo commodo condimentum nascetur.\",\n",
        "    \"Sapien auctor ad dui amet quisque. Potenti velit praesent vel conubia imperdiet. Maximus finibus consequat sapien parturient rhoncus felis. Erat placerat nisi montes; erat hac egestas quis. Bibendum metus cubilia ultricies dapibus purus cras mauris pellentesque. Accumsan turpis purus massa magna aliquam elit. Sollicitudin tempor sed est nascetur dui vestibulum lacinia tempor. Eu class est nisi metus dictumst turpis.\",\n",
        "    \"Ridiculus tempus blandit amet nunc himenaeos semper himenaeos? Imperdiet vivamus venenatis phasellus, duis tempus vulputate. Duis porta quisque nostra nulla accumsan facilisis congue? Lacus bibendum leo iaculis vulputate, cursus posuere et orci. Hendrerit risus eleifend varius torquent eros; ornare duis. Amet magna torquent euismod praesent justo nunc scelerisque aliquet scelerisque. Sed consectetur euismod praesent amet; primis consectetur montes feugiat pellentesque.\",\n",
        "    \"Quis facilisis mi magnis; nam nisi vitae. Senectus feugiat amet mattis faucibus est lobortis natoque. Auctor molestie interdum euismod, lacinia iaculis torquent gravida. Porttitor diam magna integer vivamus pellentesque quam tempus. Pellentesque tempor interdum adipiscing rhoncus dui vivamus. Commodo habitasse suspendisse, blandit vestibulum placerat sed nec. Facilisis et tristique maecenas sollicitudin ac convallis congue ipsum. Turpis sem convallis primis ipsum morbi porta pellentesque et.\",\n",
        "    \"Quis id eleifend luctus facilisis nec phasellus. Dis turpis condimentum, sodales primis luctus non potenti. Est massa fermentum interdum sapien etiam pharetra habitant. Facilisi porta dolor faucibus dis facilisi. Porta aliquam fermentum ad eu auctor blandit finibus luctus. Sed ac urna vestibulum velit habitant. Mus varius purus dui eu bibendum commodo vulputate; sed maximus. Condimentum magnis enim facilisi tellus amet dictumst. Bibendum risus porta phasellus natoque lacus vel iaculis euismod.\",\n",
        "    \"Varius cubilia enim sodales a proin lacus sagittis. Lacinia aliquam dui eleifend urna erat. Sollicitudin lorem sodales vivamus, duis adipiscing in odio. Ultricies ante efficitur penatibus blandit in. Eget sollicitudin risus dolor elit aliquet placerat porttitor arcu ad. Tempus orci habitant suspendisse a nisi ac ad purus. Molestie mollis sociosqu sollicitudin vitae ipsum amet.\",\n",
        "    \"Lacus diam risus primis taciti tempus risus odio sociosqu! Ad vehicula litora netus taciti; augue commodo tristique. Tellus platea integer torquent imperdiet ultrices tempor. Tempus montes malesuada etiam fermentum primis dui torquent litora. Maximus feugiat aptent natoque metus taciti et. Nulla urna netus ac diam nostra duis. Sagittis aliquet nunc placerat volutpat nisi egestas tincidunt. Montes laoreet venenatis et sagittis tincidunt aliquet accumsan.\",\n",
        "    \"Iaculis nunc faucibus vehicula; sociosqu etiam nibh consectetur arcu. Metus mattis gravida parturient netus posuere id. Porttitor habitant turpis odio elementum tincidunt habitant. Primis augue curabitur convallis ac nec neque cubilia. Varius nostra elit orci platea facilisi montes orci. Neque nulla convallis orci aptent placerat volutpat. Dolor nullam scelerisque tellus et proin imperdiet efficitur donec.\",\n",
        "    \"Molestie sodales class metus; sociosqu fringilla consequat. Auctor magnis urna ac volutpat; libero orci duis. Nullam platea parturient nascetur dictumst odio nunc viverra suscipit. Maecenas lobortis aliquet dolor commodo pellentesque cras. Eros vitae posuere nostra iaculis tortor est. Elementum mollis imperdiet nunc praesent iaculis sed mi. Felis sollicitudin lorem habitant magna fusce commodo enim euismod.\",\n",
        "    \"Duis in eleifend feugiat fermentum ac. Molestie nostra malesuada maximus habitasse quisque ornare. Adipiscing faucibus proin taciti facilisi dolor primis molestie id. Praesent mi integer libero viverra egestas eget ac. Dui diam potenti ex ante dapibus morbi gravida. Dis mauris lorem congue arcu magnis ultrices inceptos augue. Quam ante hendrerit dignissim montes dapibus convallis fringilla fringilla. Cras feugiat congue, sollicitudin velit laoreet tempor. Vestibulum nam facilisis dictum primis sociosqu. Gravida commodo fusce eget primis praesent maecenas.\",\n",
        "    \"At quis vulputate mollis nec euismod; mi diam. Leo metus condimentum erat platea faucibus gravida. Odio finibus posuere tortor nibh laoreet; tellus ad primis. Magna condimentum suscipit nam ultrices dignissim ac ac sodales iaculis. Vestibulum eu dapibus phasellus elementum risus lobortis accumsan. Integer platea habitant hendrerit elit consectetur finibus suspendisse.\",\n",
        "    \"Cras integer nulla mauris porttitor erat ad tortor in sem. Felis eros quis nullam, tortor condimentum habitasse. Interdum vulputate class enim sapien vel volutpat felis egestas. Neque taciti cursus orci maecenas purus. Ac metus interdum risus sociosqu elementum lobortis rhoncus? Ipsum mollis tempus mollis fringilla dis habitant per. Egestas natoque mollis habitant fusce risus interdum litora aptent litora.\",\n",
        "    \"Rutrum viverra viverra euismod duis vel, vivamus potenti facilisis. Blandit volutpat enim porta ligula sem. Semper senectus sit cursus sem vivamus elementum fermentum. Dolor nullam risus maecenas, cursus urna vestibulum. Eget convallis fermentum tristique adipiscing torquent congue mus. Malesuada aliquet hendrerit varius lectus magna hendrerit maecenas. Maximus dolor maximus risus habitant vehicula augue. Massa iaculis dui bibendum dui hendrerit libero senectus.\"\n",
        "    \"Fusce velit porttitor quam lacus libero orci, gravida lectus? Nam consectetur mi ante vestibulum id, ultrices sagittis. Dignissim imperdiet lectus vulputate neque condimentum rutrum imperdiet senectus dis. Sociosqu taciti ultrices porta amet inceptos donec; eros phasellus efficitur. Netus posuere maximus fusce cras tortor quis lobortis. Eget finibus fringilla porttitor tempor nunc inceptos. Aliquam class congue suscipit auctor egestas nibh cubilia nostra. Luctus nascetur sem purus nunc pulvinar class erat ultrices magna? Pharetra cubilia tempor curabitur dignissim himenaeos. Dapibus felis non id ac nibh maecenas ante lobortis auctor.\",\n",
        "    \"Class finibus porttitor urna fames; at laoreet ut. At tempor rhoncus praesent feugiat himenaeos tincidunt metus pellentesque nostra. Donec feugiat placerat imperdiet viverra; integer faucibus cubilia. Nam platea a commodo in sagittis dis montes aptent. Suscipit sagittis netus nisl nibh amet; placerat potenti odio. Sem hendrerit magnis integer libero nullam donec. Fermentum curae ipsum purus purus inceptos, quis curae velit sed. Ac sem luctus ultrices; iaculis elit hendrerit aliquam. Morbi blandit posuere mauris sapien sodales cubilia.\",\n",
        "    \"Suscipit eu curae rhoncus feugiat tellus ornare. Duis efficitur neque accumsan litora arcu nec lobortis. Purus quam interdum sollicitudin sit sodales commodo. Vitae nullam dolor, maecenas parturient vel urna per. Quam dictum pharetra bibendum fames lacinia cursus. Finibus lobortis placerat accumsan mattis non ante luctus imperdiet. Dui mauris rhoncus purus cubilia nulla fames eu parturient. Parturient nisl gravida facilisis convallis posuere duis sollicitudin. Curae molestie platea tempus odio justo lorem vestibulum vel.\",\n",
        "    \"Nostra interdum phasellus lectus parturient congue. Ut ultricies ut maximus habitant sodales. Urna maximus fusce molestie bibendum, consectetur luctus litora sapien. Elit quam lobortis hac sollicitudin netus risus. Sed per parturient at mattis parturient laoreet volutpat lacinia. Lectus quis lobortis ac habitasse congue pretium. Sem natoque ex proin platea morbi purus suscipit vehicula. Volutpat justo quis; augue nostra luctus lacus fames. Sit quam dolor, risus pulvinar mus nibh.\",\n",
        "    \"Nec blandit purus elementum vitae; finibus cubilia nibh. Tristique netus potenti quis cras sed; fames magna. Dictum venenatis velit sed dignissim bibendum accumsan iaculis. Quisque eget id maecenas ultrices sociosqu. Ultrices maecenas mi viverra, mi primis blandit porttitor scelerisque. Integer id adipiscing sit parturient erat conubia purus blandit. Posuere arcu mattis nullam tellus quisque eleifend facilisis est. Purus blandit aenean potenti integer habitant. Quisque parturient senectus volutpat accumsan sociosqu duis non.\",\n",
        "    \"Risus venenatis sagittis cubilia mi donec sed commodo primis. Efficitur dapibus mi scelerisque eget viverra hac tristique. Cras feugiat platea nisi justo habitasse; habitasse cras viverra. Habitasse sociosqu pellentesque neque nulla vel pellentesque. Dis malesuada consectetur vitae eget vel vitae nulla lorem fames. Sollicitudin quam eros suspendisse sollicitudin per curabitur rhoncus eget? Sodales nisi vulputate velit aliquet; dignissim non sem. Luctus dapibus pulvinar tellus inceptos ex lorem risus purus.\",\n",
        "    \"Semper mollis phasellus faucibus dis magna libero sem. Urna blandit congue sed cursus adipiscing fringilla gravida ex? Consequat diam hendrerit eleifend amet litora nam malesuada. Conubia accumsan erat duis porttitor malesuada pulvinar. Ligula velit ad eu suspendisse sollicitudin finibus pulvinar ullamcorper. Cursus ad viverra platea vivamus vestibulum senectus purus nullam. Auctor inceptos venenatis lacus et elementum. Vulputate elementum lectus efficitur sem; venenatis habitasse potenti potenti neque.\",\n",
        "    \"Lectus porta dis natoque ligula; aliquet orci eleifend. Vel maximus dignissim leo turpis parturient imperdiet nisi proin. Ut a cursus conubia eros eleifend. Aptent consectetur porta pretium integer elementum. Eleifend magna elementum himenaeos natoque nec posuere inceptos habitasse lobortis. Fermentum ad sapien sit placerat pretium faucibus. Vivamus varius potenti cubilia habitant aenean diam. Ridiculus potenti nascetur at iaculis aptent pellentesque quis.\",\n",
        "    \"Lorem enim suspendisse sagittis euismod nullam sodales. Mollis tortor conubia mus dapibus vehicula nulla curae est. Ornare efficitur metus cursus egestas volutpat enim! Aptent odio tortor aptent eu consectetur donec tempus. Adipiscing hendrerit habitasse ridiculus cubilia aptent hac varius ante. Ante accumsan consequat magna, tortor himenaeos habitasse lectus.\",\n",
        "    \"Hendrerit tellus interdum ante, sagittis sed finibus sociosqu. Diam magna metus cubilia tempus odio; hac natoque. Massa vivamus magnis volutpat pulvinar ultrices sit sit curabitur. Fermentum facilisi vulputate amet lacinia tempus morbi penatibus adipiscing. Tempor tempus ornare adipiscing nostra facilisis mi cursus enim. Placerat consectetur dictum ac risus efficitur sociosqu. Placerat ipsum duis nec consequat maecenas interdum platea taciti.\",\n",
        "    \"Neque sodales fermentum senectus eleifend auctor velit. Suspendisse quis commodo mauris vulputate in hendrerit ac nisl. Ac mollis mattis mattis orci sit parturient arcu vulputate. Vehicula aliquam aliquam et rhoncus tortor orci facilisis. Lectus malesuada tristique magna duis etiam. Ac placerat phasellus et suspendisse sodales ante varius libero elit. Varius ultricies mi gravida risus elit metus eu ac.\",\n",
        "    \"Volutpat facilisis ipsum accumsan lobortis enim commodo purus. Suspendisse habitant ullamcorper donec vel commodo dignissim non senectus. Quisque quis nascetur urna odio ac dolor iaculis sollicitudin. Lorem vulputate consequat convallis consectetur eleifend ullamcorper mollis. Facilisis felis neque nisl duis sodales, pellentesque venenatis sollicitudin. Platea ad justo, aliquet amet montes aptent platea. Pellentesque cursus convallis tortor ipsum venenatis sollicitudin donec dis duis. Suscipit vulputate penatibus eros dis lacinia fusce ipsum sagittis class. Ipsum ut massa ullamcorper ullamcorper morbi; urna lacinia luctus semper.\",\n",
        "    \"Adipiscing conubia ex; iaculis magna rhoncus nam ex faucibus. Donec lectus id convallis netus ornare sociosqu tristique proin convallis. Eros nulla consequat iaculis facilisi himenaeos porta himenaeos etiam parturient. Torquent aliquam bibendum pellentesque litora blandit quis. Cras quisque faucibus suscipit leo luctus ultricies. Integer enim pretium donec sollicitudin tincidunt ornare molestie purus. Habitant eu egestas id, ornare malesuada diam. Efficitur praesent vitae nunc tincidunt massa etiam sem risus mi.\",\n",
        "    \"Fringilla proin quis nascetur iaculis nisl non quam taciti. Est nostra bibendum sodales, vulputate turpis consequat litora. Mus libero luctus cras primis lectus molestie etiam. Faucibus mollis mauris accumsan tellus dolor in. Turpis pellentesque dictum netus hac duis elit odio. Donec quam eu finibus, mauris nam porta. Suscipit iaculis porta euismod sapien interdum tristique odio fames. Viverra laoreet ultrices lorem maecenas consectetur commodo curae egestas. Tristique feugiat dictum morbi nascetur magna scelerisque ut.\",\n",
        "    \"Torquent porttitor maximus conubia aptent taciti. Nascetur ac lobortis tempor dis ullamcorper facilisi ipsum facilisis. Mauris pulvinar quis dis tellus massa mi velit. Torquent auctor habitant at fusce convallis. Fames accumsan maecenas tempus urna lorem dictumst tellus magnis. Adipiscing taciti augue convallis sagittis quam proin.\",\n",
        "    \"Rhoncus fermentum lacus primis a euismod penatibus sed ullamcorper. Pretium amet habitasse gravida eros torquent imperdiet volutpat venenatis. Consectetur lectus turpis condimentum auctor sodales quam ipsum class lacinia. Consequat taciti viverra eleifend hendrerit eleifend turpis inceptos platea. Purus lacinia eget faucibus himenaeos ipsum eleifend. Nostra leo arcu pretium efficitur ante porttitor sapien sed? Feugiat conubia adipiscing ornare dignissim erat torquent; curabitur facilisis.\",\n",
        "    \"Scelerisque maximus vivamus dui pulvinar nostra. Suscipit commodo molestie sapien a justo tristique. Sollicitudin interdum semper potenti torquent mollis sit hendrerit rhoncus. Pretium tortor et lacus vulputate faucibus dignissim. Hac potenti venenatis pharetra risus morbi. Ac quam fames duis mauris feugiat pharetra. Eleifend facilisi eleifend litora elit vivamus dis. Nulla et arcu nibh augue cursus nascetur risus. Sollicitudin porttitor tempor praesent faucibus aptent leo. Ac rutrum ultricies quam mattis ad quam.\",\n",
        "    \"Urna eget ullamcorper, ante curabitur eros tincidunt. Magna magna vel facilisis eu mauris neque. Semper sociosqu nec nulla curabitur dignissim eleifend urna. Maximus imperdiet leo facilisis consectetur natoque sapien eget ad. Efficitur orci mi lacinia urna fusce mattis. Integer leo nam parturient sit sociosqu sem. Nullam molestie aptent ex; massa finibus magna. Blandit nunc montes magna class, etiam justo nam.\",\n",
        "    \"Donec lectus odio consectetur dignissim dignissim varius tempus. Montes tortor lectus rhoncus blandit primis proin cras. Quisque neque augue turpis risus adipiscing orci sagittis quisque. Sociosqu leo pellentesque per inceptos, vivamus tempor ad. Proin in maecenas nibh viverra quisque lacinia porttitor consequat. Leo sed ullamcorper ultrices convallis parturient dictum integer laoreet. Turpis fames urna auctor class praesent curabitur class pellentesque. Tempus facilisi adipiscing suspendisse vulputate bibendum urna. Quam varius tortor dictum etiam etiam. Senectus pellentesque donec fermentum adipiscing urna quis velit.\"\n",
        "    \"Dis augue sollicitudin proin feugiat pretium primis lacus. Odio eleifend augue pretium volutpat accumsan, parturient molestie cursus. Luctus litora nam fames ante cursus natoque integer. Duis enim placerat urna nam vulputate, consectetur torquent consectetur. Nostra convallis enim velit, nostra duis euismod sociosqu natoque proin. Donec efficitur facilisis lobortis curabitur mi. Mattis tempus cras libero cras nullam efficitur mi.\",\n",
        "    \"Tempus habitant accumsan senectus luctus nunc diam morbi. Sit dictumst lorem ipsum urna magnis. Habitant scelerisque dui congue cursus, eget lectus. Tempor nunc mauris molestie eget ante venenatis curabitur suspendisse. Ac lacus tellus nostra magnis auctor pretium quam. Ad et massa netus ac ornare donec. Adipiscing curabitur a dignissim montes netus mi nisi. Ridiculus aliquam at; consectetur congue curabitur diam.\",\n",
        "    \"Maximus aliquam dolor mollis felis ad rhoncus. Netus quis class proin integer est rhoncus. Leo pharetra eu semper elit libero sem dapibus. Habitant accumsan inceptos urna massa etiam mollis porta non. Molestie quisque mattis feugiat; sodales vel inceptos. Habitasse vivamus dapibus in efficitur nunc. Imperdiet eleifend rutrum orci; facilisi faucibus tristique. Ultrices mattis senectus nullam ad pulvinar egestas.\",\n",
        "    \"Nisl cras nam tincidunt at ultrices. Hendrerit auctor augue, nisl tempor lobortis suscipit. Dolor rutrum quisque pharetra dis semper nostra duis. Quam diam commodo proin curabitur odio ut risus. Turpis quam sapien himenaeos class semper consectetur cras felis. Placerat placerat blandit sociosqu erat neque commodo tempus. Dui nostra amet inceptos tempor hendrerit habitasse venenatis? Habitant primis accumsan fermentum cras penatibus ut eleifend. Nisl nec lobortis sollicitudin accumsan duis tellus parturient mi.\",\n",
        "    \"Pulvinar suscipit vivamus laoreet dis facilisis. Vitae integer tempus nunc malesuada cursus. Cubilia arcu sit lectus ultrices; curabitur quisque ornare consectetur a? Magnis cubilia ridiculus nisi dictum gravida. Turpis facilisis vivamus magna nisi molestie id. Phasellus cubilia interdum ad lorem tristique venenatis mus nibh. Aptent elit vulputate tempus tempus fringilla magnis enim ante.\",\n",
        "    \"Nunc imperdiet habitasse arcu ad congue. Montes mi iaculis molestie dictum himenaeos, porttitor parturient vulputate. Vel odio nisi gravida, velit at primis pellentesque erat. Condimentum nunc felis platea morbi a praesent. Tempus class per habitasse praesent platea phasellus vulputate interdum. Non taciti pharetra elementum sagittis enim nunc. Tellus pellentesque sagittis imperdiet fermentum ornare.\",\n",
        "    \"Volutpat posuere ac duis tellus accumsan massa mus. Risus arcu primis cras conubia tellus imperdiet aliquet class turpis. Ex lorem leo faucibus purus accumsan; curabitur condimentum. Elementum sem enim turpis penatibus mi maximus. Eu finibus justo posuere penatibus accumsan sit laoreet. Dapibus nullam rutrum cubilia donec imperdiet curae. Per eleifend cursus rhoncus venenatis nostra. Posuere dictumst ante sapien nullam neque ac molestie.\",\n",
        "    \"Scelerisque in vehicula elit malesuada cubilia bibendum erat nibh. Amet non ultricies, platea nec faucibus class? Maximus mauris faucibus ullamcorper facilisi laoreet. Acommodo netus fusce ut per. Est eleifend praesent turpis egestas diam ullamcorper nam. Mauris scelerisque massa metus lorem pretium libero.\"\n",
        "    \"Varius tortor torquent nullam enim pellentesque curae. Mollis platea duis consequat faucibus fringilla. Purus adipiscing blandit himenaeos eu erat. Diam ipsum pretium lobortis euismod in integer. Nec habitasse quam; dui potenti ligula dolor ullamcorper porta donec. Platea nulla nulla diam torquent praesent ut sem. Vel nunc hac consectetur platea ac libero. Diam nisl maximus rutrum dapibus leo. Nostra per sed facilisis conubia posuere, volutpat ut efficitur. Id condimentum porttitor venenatis euismod curabitur.\",\n",
        "    \"Accumsan habitasse porttitor lectus placerat integer vitae laoreet litora quam. Penatibus diam euismod mi varius lacinia sociosqu. Viverra orci laoreet venenatis cursus morbi amet iaculis eget orci. Iaculis cursus congue donec imperdiet penatibus aptent. Sapien primis quam primis lacus neque torquent suspendisse. Massa risus a ipsum mauris porta dictumst.\",\n",
        "    \"Praesent vivamus rhoncus erat potenti cursus nulla. Ex eros eu morbi dignissim tempor bibendum. Elit magna laoreet nibh duis dictum aptent. Placerat mollis aenean augue quam class fringilla? Hendrerit potenti cras suspendisse sodales tempus leo. Blandit aptent consequat class lacinia ex. Himenaeos sociosqu sapien volutpat parturient velit laoreet venenatis maximus.\",\n",
        "    \"Ad magnis etiam malesuada interdum orci cras consequat ex? Laoreet nisl cras fermentum orci donec. Euismod luctus consequat venenatis conubia augue eu dapibus bibendum lorem. Orci fames libero et quis integer bibendum. Feugiat pretium convallis quam sociosqu himenaeos. Varius iaculis tortor gravida habitant nostra, maximus facilisis? Turpis mi mollis ex aliquam finibus nam. Potenti metus nunc tellus sed facilisi.\",\n",
        "    \"Odio metus sollicitudin laoreet nascetur curae magna. Inceptos ipsum velit senectus luctus; luctus vel. Mi maecenas sapien egestas primis ipsum sem ullamcorper. Pretium malesuada mus nam aliquam aliquet cras dictum proin nec. Parturient fames tristique vehicula scelerisque et. Penatibus porttitor lacus tellus interdum condimentum ridiculus parturient. Congue dui natoque sit tristique lacinia semper ut duis. Semper ipsum ornare neque et aliquam. Vitae lobortis praesent cursus iaculis quisque dolor mauris.\",\n",
        "    \"Proin facilisi erat natoque magna hac vel fringilla. Et netus mus suscipit tortor ultrices! Auctor quam fusce vulputate feugiat ante. Faucibus class in ultricies ex tincidunt lobortis, tempus posuere. Eu nisi etiam himenaeos, tincidunt massa congue litora justo. Vitae sagittis aliquam ornare conubia ornare eu. Nibh congue fusce vulputate cubilia; in odio habitasse et. Egestas neque ridiculus ridiculus conubia eu lacinia sagittis metus.\",\n",
        "    \"Arcu semper sagittis eu torquent netus imperdiet per. Curae sagittis maecenas sociosqu aptent varius cursus cras. Amet accumsan conubia ultrices id blandit! Placerat consequat bibendum lobortis molestie justo nec interdum sollicitudin. Dui lectus enim pharetra placerat nulla massa rhoncus. Scelerisque sem finibus lacinia dolor dui. Dignissim euismod sem; id faucibus maximus potenti tristique ante. Ad et curae pretium odio etiam penatibus. Primis arcu finibus lectus orci ridiculus.\",\n",
        "    \"Nec dictum ullamcorper nostra; euismod nam maecenas justo erat. Facilisis aliquam finibus neque aenean proin sit id. Vel ligula mi ultricies dictum nostra scelerisque lacinia. Imperdiet sit curabitur erat nisl arcu semper. Condimentum quisque morbi montes blandit mauris rhoncus interdum scelerisque adipiscing. Odio magnis egestas cursus sed urna ipsum hac dis ipsum. Pretium adipiscing conubia nostra vestibulum arcu nullam class. Parturient mi ultrices metus libero pulvinar efficitur. Turpis bibendum vulputate nascetur, sem interdum pretium sociosqu.\",\n",
        "    \"Risus id blandit praesent mattis rhoncus aliquet. In ullamcorper sagittis ex; molestie curae ultrices hac nisl. Bibendum lorem parturient maecenas rutrum, metus fames. Quis at condimentum elementum ligula penatibus mollis. Aliquet facilisi pharetra suspendisse tortor augue risus eleifend finibus proin? Non aptent maecenas vulputate per eros ultrices. Pellentesque habitant cras quam blandit sit non.\",\n",
        "    \"Primis ornare per tincidunt, cras scelerisque parturient parturient penatibus. Senectus maximus cubilia venenatis quis sit integer natoque pulvinar rutrum? Montes enim quis tellus malesuada risus. Quam ullamcorper habitasse in curae elementum tincidunt. Eleifend eget habitant etiam blandit penatibus felis adipiscing porttitor. Libero non metus posuere nulla fames malesuada at ridiculus felis. At lectus eget morbi sit facilisi non dis sed. Consectetur porttitor blandit tempor pharetra, neque et phasellus.\",\n",
        "    \"Fusce molestie iaculis hac aliquet hac dolor bibendum. Dignissim iaculis lobortis senectus dapibus magna aptent. Vivamus nam condimentum at lacinia id a leo viverra vestibulum. Blandit nisl ipsum vulputate fringilla sagittis pellentesque blandit. Ex primis himenaeos vivamus congue massa nostra velit. Ac eget dictum porta potenti ut libero eget. Class senectus sit purus bibendum nisi. Nam libero conubia euismod consectetur lorem adipiscing enim.\",\n",
        "    \"Pharetra habitant risus eros tortor magnis suscipit. Rhoncus et sociosqu lectus velit felis elementum tincidunt lacinia maecenas. Platea blandit blandit duis tempor urna ex mattis sapien aptent. Suscipit convallis habitant nascetur ultricies torquent dignissim dolor ex. Convallis nam malesuada aliquam habitasse mollis placerat est. Netus phasellus libero tristique etiam convallis venenatis, nostra id.\",\n",
        "    \"Congue orci quisque sed venenatis adipiscing sodales dictum placerat eget. At habitant lorem bibendum lacus blandit fames ipsum nulla. Ornare magna semper sagittis, varius non efficitur sollicitudin. Massa orci neque erat sit egestas montes enim. In massa dui ut nulla nisl hac sodales at neque. Inceptos ipsum consequat suspendisse ullamcorper rhoncus pulvinar. Ipsum ultricies molestie hendrerit sit vitae leo dis.\",\n",
        "    \"Ultrices ac placerat magnis eros nibh elementum. Suscipit duis ut diam taciti sociosqu dui. Urna dignissim fames nostra interdum condimentum erat. Ipsum donec natoque id integer imperdiet consequat pulvinar sem. Potenti cubilia nostra; pellentesque elit odio dignissim commodo maecenas. Mi cubilia enim condimentum per torquent magna conubia porta. Etiam orci justo faucibus arcu mollis semper laoreet donec.\",\n",
        "    \"Senectus taciti fringilla ante maecenas commodo scelerisque mus ipsum. In posuere hendrerit ligula sit tellus. Ex ultricies egestas risus, lorem mauris litora. Sem vel torquent porta tellus neque interdum dolor lacus. Nullam euismod tristique fames sociosqu magna. Dictum consequat et quam, fusce libero enim iaculis. Cras ultrices facilisis suscipit sit tempor ultrices sociosqu. Arcu ex congue himenaeos nisi hac elementum facilisi sapien.\",\n",
        "    \"Elementum donec parturient consequat velit accumsan libero tincidunt. Et aliquet dui ullamcorper, ac eleifend pellentesque vivamus. Aliquam turpis eget eros fermentum pretium felis laoreet ornare. In magna sollicitudin blandit tincidunt nec malesuada. Hendrerit nam per tincidunt fringilla fringilla elit. Mauris nunc amet tincidunt, luctus aliquam eros ornare ridiculus. Etiam tincidunt habitasse consequat pulvinar risus cubilia lobortis euismod amet. Sed scelerisque libero purus accumsan posuere fermentum ullamcorper. Urna eros sollicitudin himenaeos bibendum pellentesque. Leo fringilla semper conubia dis platea integer laoreet eu suscipit.\",\n",
        "    \"Interdum cras faucibus neque accumsan conubia diam eget. Justo adipiscing ut euismod class eget lobortis. Dapibus donec mi nec porta; maximus donec. Facilisis vel ullamcorper phasellus eros aliquet enim faucibus. Tempor turpis cras vel euismod ante. Fusce purus natoque lacinia eleifend odio. Dignissim cursus nam tortor porta turpis nisi.\",\n",
        "    \"Praesent cursus faucibus natoque fames finibus a eget egestas. Aliquet accumsan magna hac porttitor praesent cubilia ullamcorper. Amet curae fermentum facilisis eu ante urna curabitur. Dignissim efficitur inceptos finibus ante venenatis venenatis vivamus lacinia. Posuere dictum posuere pulvinar, nisi fames fusce luctus. Libero semper montes pharetra hendrerit massa fringilla; odio sem ridiculus. Praesent elit congue ipsum sollicitudin lacinia a. Accumsan elementum maecenas tristique eget a suscipit nisl gravida.\",\n",
        "    \"Dis dapibus non diam eros finibus ad imperdiet. Sagittis vivamus tellus; pretium luctus torquent hendrerit. Arcu mollis libero justo, varius sit felis penatibus. Nunc inceptos dictum, convallis non congue posuere morbi. Suscipit litora dapibus vulputate; semper fusce lacinia. Viverra magnis vulputate pretium phasellus odio viverra. Gravida mus taciti cras vivamus augue.\",\n",
        "    \"Ligula consequat cubilia enim enim suspendisse phasellus ac efficitur. Magna commodo accumsan finibus proin hac vestibulum habitant. Massa inceptos mus phasellus sit curabitur platea? Condimentum ultricies sem finibus himenaeos pulvinar quis cursus nibh condimentum. Porta sed potenti eu potenti ad mollis pellentesque maximus. Mollis eleifend tempus varius potenti nec dis integer.\",\n",
        "    \"Ex aliquam duis, quis vitae quam molestie. Aliquam dapibus dolor laoreet primis cursus odio penatibus. Elementum potenti eu dis feugiat; velit dignissim. Tincidunt nam vivamus pellentesque, suscipit dui ante. Arcu purus sit, sit gravida mollis consectetur varius justo amet. Placerat proin ornare mollis ridiculus eget a. Fringilla egestas urna aenean habitasse, aliquet natoque luctus. Enim nostra lorem rutrum in eleifend bibendum hendrerit.\",\n",
        "    \"Orci a ullamcorper luctus vivamus sodales blandit eleifend nam. Nostra nascetur justo lacus, ultrices elit suspendisse. Varius non platea eu consectetur massa himenaeos. Varius nullam vestibulum taciti vivamus erat fusce ornare sit. Sed nisi nulla potenti, sagittis elementum condimentum euismod finibus. Et torquent scelerisque velit diam, dolor tristique eget efficitur. Posuere aptent imperdiet aliquam est congue ex purus faucibus magna. Tellus taciti vulputate mollis justo molestie donec. Suscipit fringilla litora porttitor aenean magnis ultricies class.\",\n",
        "    \"Tortor sociosqu sed convallis mus etiam tincidunt curae. Interdum elementum odio vitae accumsan porttitor efficitur arcu. Senectus bibendum vel at auctor fames convallis arcu. Nullam ut sit elementum molestie orci velit penatibus. Dis elementum ex augue interdum nascetur finibus. Nascetur pharetra urna euismod eleifend; ultricies lobortis nostra. Condimentum fusce etiam habitant; facilisi vulputate lobortis. Molestie lobortis habitasse elementum nec orci. Sodales lorem bibendum molestie per adipiscing montes cubilia.\",\n",
        "    \"Eleifend quis ac mauris ad curabitur fringilla. Fringilla massa eu taciti aliquam consequat maecenas lacus donec est. Primis nostra aptent at mattis primis convallis urna consectetur? Potenti habitant gravida ridiculus sapien torquent ad et quam fames. Quam morbi vehicula risus imperdiet dolor est. Efficitur massa condimentum habitant maecenas viverra!\",\n",
        "    \"Mattis mauris curae platea potenti efficitur scelerisque ultrices parturient. Facilisis cubilia non, nisl egestas libero nostra. Malesuada primis consectetur lacinia proin porta inceptos netus mauris duis. Quis risus nunc ridiculus nostra faucibus aenean volutpat metus vulputate. Vestibulum efficitur auctor ligula phasellus fames suspendisse. Urna nisi sollicitudin purus class ultrices. Orci aenean rutrum turpis auctor luctus massa iaculis nisi. Vestibulum leo sed convallis senectus vulputate faucibus facilisi. Suspendisse vel sollicitudin dapibus lobortis lobortis himenaeos suspendisse lacus. Vestibulum porttitor class mus sodales tellus arcu erat.\",\n",
        "    \"Dis libero nec accumsan massa scelerisque; cras augue diam. Nullam urna eleifend augue metus lobortis elit congue? Sem magnis odio tortor est porta ipsum; sagittis fermentum. Placerat conubia habitasse non dapibus semper accumsan magna vestibulum tellus. Curabitur fusce aliquam euismod quam libero purus luctus molestie. Mollis turpis proin senectus fringilla netus dapibus habitasse. Non curabitur ad elit porta mi fames malesuada. Eros mi tempor porttitor per quisque eleifend nam cras. Pharetra primis ligula pulvinar metus pretium.\",\n",
        "    \"Viverra eleifend consectetur metus nascetur dolor scelerisque accumsan etiam. Cursus bibendum duis odio; nisl velit proin. Non enim laoreet sit; malesuada auctor torquent. Lacus nisi malesuada quam integer natoque dignissim. Duis consectetur pharetra libero faucibus ac volutpat ac. Dis condimentum tempus congue eros dignissim sollicitudin magnis auctor. Dignissim facilisis ligula mi morbi morbi et torquent a.\",\n",
        "    \"Dolor ipsum euismod imperdiet taciti viverra eget suspendisse. Nam luctus platea, auctor sociosqu cras placerat suscipit. Nibh praesent pharetra pharetra torquent quisque. Leo primis a sociosqu curae himenaeos et habitant posuere. Et maximus ligula, nam nostra etiam varius? Pharetra nam rutrum eu fames, tincidunt semper.\",\n",
        "    \"Faucibus porta ad euismod sem ad leo conubia. Vivamus porta dis bibendum hac mus sem. Hendrerit natoque non et montes efficitur vulputate orci. Urna maximus curae mus eleifend potenti leo. Hac tortor leo augue consequat pellentesque. Purus tristique semper ornare vulputate auctor molestie turpis donec. Proin habitant facilisis magnis praesent tincidunt conubia tristique. Nulla quam congue malesuada tortor efficitur egestas class nostra.\",\n",
        "    \"Consectetur in tortor gravida fringilla senectus auctor lobortis sodales. Dictum et magnis proin massa imperdiet. Bibendum fermentum ex scelerisque lacinia posuere tempus integer ante et. Integer phasellus et augue ullamcorper sociosqu dignissim class turpis. Condimentum pretium ornare justo ultricies leo in. In dolor adipiscing vel mattis ante hendrerit ullamcorper. Non ipsum odio elementum donec libero lobortis suscipit elementum fusce.\",\n",
        "    \"Sollicitudin gravida conubia himenaeos eleifend nascetur est ipsum? Lacinia posuere dui auctor adipiscing; curae facilisi dapibus. Suspendisse turpis cras ridiculus velit vivamus quis. Pellentesque curae vulputate pretium scelerisque et. Suspendisse aliquam gravida convallis proin taciti sollicitudin finibus. Congue tempus sodales nec ornare ultricies mollis. Eleifend porttitor elit fusce parturient porta sem. Hac montes dictum nisl pulvinar bibendum eget.\",\n",
        "    \"Mi maecenas conubia etiam libero lectus pretium tincidunt semper. Praesent eros metus a luctus ut diam neque nunc luctus. Fringilla rutrum interdum ullamcorper fringilla habitasse aptent curabitur hendrerit elit. Sociosqu senectus duis netus senectus parturient pretium adipiscing sagittis. Curae varius lacinia nunc pulvinar arcu est dolor pellentesque. Porta torquent dictumst porttitor platea habitant feugiat. Lobortis penatibus tortor cursus, nisl dapibus nullam tincidunt. Faucibus egestas ligula augue parturient sed sagittis facilisi. Maecenas convallis ante adipiscing sollicitudin ipsum leo imperdiet mollis.\",\n",
        "    \"Facilisi nascetur lacus efficitur tempor habitant eros commodo. Taciti phasellus ullamcorper turpis amet aenean. Vel tristique eleifend consectetur, eros dis mattis. Donec magnis nibh tortor felis; facilisi fames turpis malesuada. Efficitur eget himenaeos mattis; scelerisque mollis odio risus. Laoreet tincidunt class facilisi urna euismod scelerisque. Arcu auctor bibendum congue, nunc in vehicula rutrum aenean non.\",\n",
        "    \"Fusce lacinia maximus ullamcorper montes massa. Congue tincidunt convallis, accumsan tellus placerat netus. Dictum sociosqu neque vitae pretium vel iaculis pharetra. Natoque nec euismod tristique sociosqu quam facilisi. Volutpat ad parturient sagittis auctor lacus justo sodales. Tempus commodo nisi maecenas neque luctus ridiculus hendrerit. Magna nunc integer fusce iaculis vel primis congue magna. Mus turpis tortor mi porttitor mi.\",\n",
        "    \"Rhoncus fames pretium conubia luctus, purus varius eget mauris luctus. Dolor tellus platea erat enim aenean urna. Porttitor libero dui semper sagittis vehicula. Magnis quisque aliquam pretium aenean malesuada eleifend. Vel ligula torquent morbi mauris porta. Facilisis ullamcorper morbi vel aptent lorem.\",\n",
        "    \"Tempor ex proin cras pulvinar cubilia proin felis. Donec dapibus hac viverra orci fringilla donec. Dis aptent auctor commodo venenatis, nostra eget. Parturient velit imperdiet ullamcorper; enim habitasse aenean. Justo penatibus ac conubia lacus egestas vivamus purus. Montes ligula nascetur dui nascetur consequat posuere aliquet. Lorem vivamus bibendum velit convallis dignissim placerat parturient fusce.\",\n",
        "    \"Lorem ad duis platea consequat rutrum feugiat ex. Diam ullamcorper egestas vulputate urna ipsum egestas mattis aliquam. Cubilia nisi fusce faucibus elit porta sodales. Molestie scelerisque habitasse malesuada fames metus praesent praesent faucibus. Himenaeos curae ac maximus dui inceptos mauris hac interdum pellentesque. Urna ultricies consequat, nec hendrerit nulla senectus habitant himenaeos. Enim bibendum platea sit sagittis vestibulum eros; ligula rhoncus. Nec sollicitudin dolor, magnis aliquet viverra dis penatibus. Eget proin eleifend viverra mollis mattis penatibus.\",\n",
        "    \"Turpis tempus habitasse dapibus odio fames suscipit. Porttitor scelerisque accumsan posuere odio taciti consequat? Aultricies ultricies quam tortor ipsum platea. Diam ac elementum ornare quis tellus metus tellus. Penatibus leo rhoncus varius pharetra conubia penatibus. Amet integer laoreet eleifend ut nulla pellentesque augue dapibus. Nibh sodales tincidunt quis vivamus tellus lacinia primis sollicitudin ornare. Id elit sed mattis rutrum penatibus.\",\n",
        "    \"Efficitur nisi phasellus a mus tristique natoque et. Leo etiam sodales vestibulum luctus velit. Enim etiam volutpat blandit magnis praesent. Donec pellentesque ullamcorper turpis ex risus facilisis faucibus vivamus erat. Convallis justo sit conubia tristique mattis. Ac pharetra amet venenatis nullam vehicula dis facilisis ipsum sodales. Lacus orci netus nascetur ad cubilia aenean fames. Massa purus vehicula imperdiet eget duis ipsum? Lacus natoque habitant risus; massa cubilia ornare at.\",\n",
        "    \"Laoreet consequat vulputate nulla elit nisl dictum phasellus scelerisque. Vitae maecenas magnis tortor neque aliquam. Porttitor scelerisque dictumst dignissim curae maecenas viverra potenti vestibulum. Ipsum nulla fermentum aptent semper pellentesque mattis potenti. Nulla eu arcu aptent bibendum, pellentesque varius orci. Feugiat velit libero pellentesque nunc integer donec himenaeos a. Leo dignissim at nisi euismod elementum efficitur; tempus praesent. Aproin hac lacus sapien; rhoncus risus dapibus. Lacus donec convallis in feugiat aliquam primis.\",\n",
        "    \"Torquent penatibus fames eros parturient eu eros. Netus purus tortor nascetur netus hac nam dictum scelerisque integer. Mus pretium bibendum pharetra nostra, habitasse blandit gravida. Ac interdum imperdiet purus fermentum ultrices. Per habitasse blandit porttitor pretium enim aliquam. Congue nunc lacinia quisque maximus integer.\",\n",
        "    \"Nibh vitae ac porttitor odio nullam class diam? Curabitur magnis vulputate porta pellentesque non habitant. Tristique cursus justo iaculis dolor diam aliquet ac ut. Sodales auctor ex vivamus sed elit fusce? Phasellus fermentum maximus consequat; sed natoque ligula. Sed venenatis integer tempus est vehicula ultricies finibus tortor mauris. Luctus turpis magna natoque proin maecenas ullamcorper.\",\n",
        "    \"Vehicula facilisis netus fames egestas feugiat accumsan nibh ac suscipit. Sagittis conubia posuere congue vestibulum nostra quam nascetur himenaeos. Commodo taciti hac nullam; nullam augue orci erat. Rhoncus ipsum pretium volutpat platea accumsan. Lacus mi egestas, curae pulvinar dapibus nam augue. Eu tempor senectus urna praesent mi per aliquet gravida? Faucibus tellus neque convallis accumsan etiam consequat egestas. Proin fermentum dui curae posuere ut sapien mus. Potenti pulvinar habitasse dictum mollis montes. Sit tortor varius fusce auctor lobortis.\",\n",
        "    \"Torquent nibh sagittis posuere pretium aenean proin egestas. Conubia accumsan penatibus maximus ullamcorper arcu nostra duis conubia. Hendrerit penatibus nunc vel, efficitur enim aenean. Ipsum purus nibh facilisis quam himenaeos dapibus felis convallis? Etiam tincidunt elit torquent nascetur augue sed. Fermentum tincidunt sagittis faucibus facilisi eleifend fermentum interdum arcu eu. Ultricies sollicitudin ut himenaeos vel nisl.\",\n",
        "    \"Malesuada aliquam semper sed molestie urna. Parturient amet dapibus habitant nam malesuada. Natoque nascetur magnis ridiculus lobortis metus ultricies. Risus ultrices quisque lacinia conubia luctus nullam! Morbi cubilia ex enim venenatis nisi et. Diam aliquet habitant quam viverra platea. Tincidunt integer condimentum mollis imperdiet, faucibus tristique. Sociosqu tempor finibus etiam imperdiet vitae ex nisi.\",\n",
        "    \"Diam quisque dolor molestie nostra montes quis pellentesque. Ridiculus est fermentum et eu; odio habitasse semper taciti. Turpis sodales mauris etiam justo pretium; sapien ligula eu. Phasellus consectetur in consectetur leo condimentum nunc id. Phasellus egestas at lobortis vestibulum nibh. Accumsan velit litora cursus dignissim massa ad nulla massa nisi. Eget etiam facilisi curae ullamcorper accumsan class scelerisque. Nec etiam primis varius viverra mauris a cras hendrerit tempor.\",\n",
        "    \"Praesent tincidunt donec natoque gravida malesuada tincidunt venenatis. Venenatis nostra morbi a odio elit posuere mollis nec justo. Tincidunt eget nascetur blandit fames; curae consectetur. Tincidunt fringilla et risus id, netus ornare. Risus consequat a diam cursus tempor orci at varius. Dui cursus eget suspendisse varius purus ornare interdum potenti.\",\n",
        "    \"Ultrices mus dictumst tempus purus nec iaculis id. Ligula efficitur dis fusce pretium platea tellus potenti enim? Interdum nunc justo ac nostra turpis amet taciti habitasse? Eu ac imperdiet parturient volutpat, faucibus maecenas. Parturient interdum aliquet aenean natoque, enim himenaeos accumsan imperdiet. Morbi neque aliquet elit facilisis, justo varius venenatis. Vitae sociosqu natoque nec nostra imperdiet lacus dis quis venenatis. Litora class arcu commodo nisl bibendum congue. Curae potenti magnis aliquet lacus tempor lectus etiam mollis. Ligula feugiat lacinia sociosqu ad quisque suscipit nam nullam sem.\",\n",
        "    \"Auctor maecenas platea netus suscipit vitae at leo. Egestas est sodales egestas fermentum, elit rutrum. Tincidunt gravida laoreet quisque pellentesque eleifend quisque rutrum fames. Habitant mollis arcu tincidunt egestas tellus hac aenean. Augue euismod nullam class sapien tincidunt quam. Porta proin nisi maecenas praesent velit class felis. Interdum molestie maximus metus pulvinar auctor condimentum. Mus semper lobortis fermentum pretium sem natoque montes duis velit.\",\n",
        "    \"Primis nisi augue purus mauris mollis nascetur. Fames nisl amet nec senectus nascetur quis blandit quis urna. Porttitor mattis volutpat gravida vulputate cras? Fames ligula taciti platea euismod litora. Cras egestas sit enim volutpat cursus sagittis dictum. Senectus volutpat bibendum nostra habitasse, faucibus neque. Ultricies laoreet ultrices ipsum magnis; sodales quam amet. Lacus malesuada venenatis vulputate tempus felis; pretium ex aenean.\",\n",
        "    \"Torquent pretium donec hendrerit eu leo mi parturient cursus. Semper turpis sem rutrum egestas quis commodo. Rhoncus tempor dignissim mauris fames non sem; leo tempus. Mi hendrerit a nullam sit cursus aliquet praesent. Aporta maximus gravida porttitor purus. Etiam porttitor sagittis cursus interdum sed platea lacinia. Scelerisque velit lobortis urna; class sapien sit. Nisl ligula venenatis lectus parturient euismod nam dapibus. Dapibus pharetra neque finibus vestibulum tincidunt potenti urna tortor!\",\n",
        "    \"Lacus ac rutrum volutpat iaculis molestie. Curae ipsum etiam eu gravida odio; condimentum dolor placerat pellentesque. Cursus iaculis integer tellus porttitor vitae. Curae justo phasellus conubia odio, consectetur eros et. Ad imperdiet euismod aenean himenaeos egestas. Iaculis convallis dignissim; curabitur tempor consequat cursus aliquet massa. Proin lacus sed nunc posuere vivamus cras eros. Dictum in aliquet dictum platea quis suscipit. Quisque montes sagittis ipsum eleifend eget ultricies feugiat.\",\n",
        "    \"In odio montes parturient auctor tortor himenaeos fringilla habitasse. Dictumst quis vivamus duis quam suspendisse dapibus habitant primis a. Nascetur placerat dictum; pulvinar luctus cras sodales ipsum. Nam congue efficitur, arcu ante maximus lectus. Nisi ad facilisi ornare ex fermentum. Laoreet leo feugiat aptent; sapien pharetra augue. Tristique tempor curabitur, faucibus dictum facilisi dolor. Tristique montes ridiculus magnis ridiculus tristique curae aptent.\",\n",
        "    \"Sem metus tristique penatibus nullam a, nostra magna vivamus. In dis scelerisque nisi himenaeos quisque. Lacus conubia mus iaculis scelerisque aliquet eget. Sapien duis imperdiet fusce maximus risus. Tristique felis congue felis scelerisque sodales. Dolor purus tristique nam habitant velit molestie dictum. Velit hendrerit suscipit nisl lacinia curabitur penatibus. Facilisi dapibus pellentesque, inceptos aptent proin praesent auctor. Vel conubia augue eros accumsan mi adipiscing. Aliquam est etiam eleifend ultricies montes, eros est.\",\n",
        "    \"Luctus cursus purus bibendum aenean cubilia fusce velit convallis. Porta pharetra augue sem parturient in natoque ad. Risus erat curabitur congue tempor cubilia adipiscing? Placerat ac sem elit congue tincidunt efficitur finibus porttitor pharetra. Maximus mus litora libero vivamus tempor. Rhoncus malesuada fusce id ante proin, sed luctus donec.\",\n",
        "    \"Suscipit sem euismod himenaeos rhoncus vitae. Feugiat taciti adipiscing lobortis libero taciti maecenas sapien hac. Curae metus praesent magna habitasse facilisi id purus. Tristique lacinia nullam eros magnis turpis. Diam habitasse mollis praesent nibh; vestibulum habitasse a. Condimentum accumsan rutrum blandit etiam augue a elementum pharetra. Pretium orci porta aptent, hendrerit sit consectetur nam. Fringilla vehicula pharetra ligula penatibus efficitur natoque ex. Vehicula cras massa sollicitudin id natoque scelerisque consequat.\",\n",
        "    \"Nullam gravida accumsan dignissim inceptos vitae suscipit mi feugiat. Dis feugiat eget sodales ligula urna sodales vel. Varius metus sodales massa non; suspendisse quis laoreet. Feugiat pretium elit malesuada ex vehicula. Purus velit dapibus venenatis nec eget cursus pharetra. Parturient imperdiet integer sodales, leo orci commodo auctor. Est leo rhoncus eu nunc arcu. Eget proin arcu aliquam vel non est turpis? Suspendisse blandit aptent cubilia felis mi euismod interdum.\",\n",
        "    \"Consequat vivamus elit dis; integer penatibus risus. Torquent nisl imperdiet ut lobortis non. Integer rhoncus id dapibus, adipiscing ipsum penatibus arcu risus. Egestas ornare nostra; enim primis at tempus. Pulvinar massa facilisi vehicula vehicula ad mauris quisque! Blandit viverra suscipit vel dui urna cursus ullamcorper; libero ultrices. Lacus pretium adipiscing integer quisque turpis.\"\n",
        "]\n",
        "\n",
        "print(len(texts))"
      ],
      "metadata": {
        "id": "sDT3Rt_4yeS1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d80d75bd-e899-42a7-a287-3ce911331364"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "# Train a BPE tokenizer\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "trainer = BpeTrainer(vocab_size=4000, special_tokens=[\"[UNK]\", \"[PAD]\", \"[EOS]\"])\n",
        "tokenizer.train_from_iterator(texts, trainer)\n",
        "\n",
        "# Save and load the tokenizer\n",
        "tokenizer.save(\"bpe_tokenizer.json\")\n",
        "tokenizer = Tokenizer.from_file(\"bpe_tokenizer.json\")"
      ],
      "metadata": {
        "id": "ISHzA2CtmJeQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.scale = torch.nn.Parameter(torch.ones(dim))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        norm = torch.norm(x, dim=-1, keepdim=True)\n",
        "        return x / (norm + self.eps) * self.scale"
      ],
      "metadata": {
        "id": "gcM5PIAJkOrL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class GeGLU(nn.Module):\n",
        "    def __init__(self, dim_in, dim_out):\n",
        "        super(GeGLU, self).__init__()\n",
        "        self.proj = nn.Linear(dim_in, dim_out * 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, gate = self.proj(x).chunk(2, dim=-1)\n",
        "        return x * torch.sigmoid(gate)"
      ],
      "metadata": {
        "id": "Tp7gGZ56kQKN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class GroupedAttentionLayer(nn.Module):\n",
        "    def __init__(self, dim, num_heads, num_groups, max_seq_len, device='cpu'):\n",
        "        super(GroupedAttentionLayer, self).__init__()\n",
        "        self.dim = dim\n",
        "        self.num_heads = num_heads\n",
        "        self.num_groups = num_groups\n",
        "        self.head_dim = dim // num_heads\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.device = device\n",
        "\n",
        "        # Linear projections for Q, K, V\n",
        "        self.q_linear = nn.Linear(dim, dim)\n",
        "        self.k_linear = nn.Linear(dim, dim)\n",
        "        self.v_linear = nn.Linear(dim, dim)\n",
        "        self.out_linear = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, dim = x.shape\n",
        "\n",
        "        # Precompute the RoPE embeddings\n",
        "        rope_sin, rope_cos = create_1d_rope_embeddings(seq_len, self.head_dim, self.device)\n",
        "\n",
        "        # Linear projections\n",
        "        q = self.q_linear(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        k = self.k_linear(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        v = self.v_linear(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Apply RoPE to Q, K, V\n",
        "        q = apply_1d_rope(q, rope_sin, rope_cos)\n",
        "        k = apply_1d_rope(k, rope_sin, rope_cos)\n",
        "        v = apply_1d_rope(v, rope_sin, rope_cos)\n",
        "\n",
        "        # Group the queries, keys, and values\n",
        "        q = q.view(batch_size, self.num_heads, self.num_groups, seq_len, self.head_dim // self.num_groups)\n",
        "        k = k.view(batch_size, self.num_heads, self.num_groups, seq_len, self.head_dim // self.num_groups)\n",
        "        v = v.view(batch_size, self.num_heads, self.num_groups, seq_len, self.head_dim // self.num_groups)\n",
        "\n",
        "        # Compute attention using scaled_dot_product_attention\n",
        "        attn_output = []\n",
        "        for group in range(self.num_groups):\n",
        "            attn_output.append(F.scaled_dot_product_attention(\n",
        "                q[:, :, group], k[:, :, group], v[:, :, group], attn_mask=None, dropout_p=0.0, is_causal=True\n",
        "            ))\n",
        "        attn_output = torch.cat(attn_output, dim=2)\n",
        "\n",
        "        # Reshape the output\n",
        "        out = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, dim)\n",
        "\n",
        "        # Linear projection\n",
        "        out = self.out_linear(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class TransformerLayer(nn.Module):\n",
        "    def __init__(self, dim, num_heads, num_groups, max_seq_len, device='cpu'):\n",
        "        super(TransformerLayer, self).__init__()\n",
        "        self.attention = GroupedAttentionLayer(dim, num_heads, num_groups, max_seq_len, device)\n",
        "        self.norm1 = RMSNorm(dim)\n",
        "        self.norm2 = RMSNorm(dim)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(dim, dim * 4),\n",
        "            GeGLU(dim * 4, dim * 4),\n",
        "            nn.Linear(dim * 4, dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply attention and add & norm\n",
        "        x = self.norm1(x + self.attention(x))\n",
        "        # Apply feed-forward network and add & norm\n",
        "        x = self.norm2(x + self.ffn(x))\n",
        "        return x\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, num_layers, dim, num_heads, num_groups, max_seq_len, vocab_size, device='cpu'):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, dim)\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerLayer(dim, num_heads, num_groups, max_seq_len, device)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.ln_f = RMSNorm(dim)\n",
        "        self.head = nn.Linear(dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Token embeddings\n",
        "        x = self.token_embedding(x)\n",
        "        # Apply transformer layers\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        # Final layer norm\n",
        "        x = self.ln_f(x)\n",
        "        # Output logits\n",
        "        logits = self.head(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "CuP6A_EFmQea"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Tokenize the texts and ensure tensors are of type torch.long\n",
        "encoded = tokenizer.encode_batch(texts)\n",
        "sequences = [torch.tensor(enc.ids, dtype=torch.long) for enc in encoded]\n",
        "\n",
        "# Pad sequences to the same length with explicit dtype\n",
        "pad_token_id = tokenizer.token_to_id(\"[PAD]\")\n",
        "seq_len = max(len(seq) for seq in sequences)\n",
        "sequences_padded = [torch.cat([seq, torch.tensor([pad_token_id] * (seq_len - len(seq)), dtype=torch.long)]) for seq in sequences]\n",
        "\n",
        "# Create input and target tensors with dtype=torch.long\n",
        "inputs = torch.stack(sequences_padded)\n",
        "targets = torch.stack([torch.cat([seq[1:], torch.tensor([pad_token_id], dtype=torch.long)]) for seq in sequences_padded])\n",
        "\n",
        "dataset = torch.utils.data.TensorDataset(inputs, targets)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=128, shuffle=True)"
      ],
      "metadata": {
        "id": "ZBWMndCNmVo_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(inputs[5].squeeze().tolist(), skip_special_tokens=False))\n",
        "print(tokenizer.decode(targets[5].squeeze().tolist(), skip_special_tokens=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGF8ZjhDWxvW",
        "outputId": "64066b77-cba6-402e-de0a-b6fc14374263"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Efficitur aptent ac viverra ullamcorper eget sollicitudin eget cursus . Erat bibendum penatibus praesent quam sollicitudin . Ante quam faucibus mus volutpat phasellus penatibus . Senectus scelerisque turpis mus feugiat viverra morbi sociosqu . Class nec placerat ex , a hac nec . Curabitur quis fermentum tincidunt neque augue lacinia . Aex conubia ex vivamus ligula proin sit . Bibendum habitasse cursus congue morbi platea . Taciti est finibus accumsan iaculis montes rutrum . Consequat vulputate odio conubia sociosqu imperdiet porta ! [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "aptent ac viverra ullamcorper eget sollicitudin eget cursus . Erat bibendum penatibus praesent quam sollicitudin . Ante quam faucibus mus volutpat phasellus penatibus . Senectus scelerisque turpis mus feugiat viverra morbi sociosqu . Class nec placerat ex , a hac nec . Curabitur quis fermentum tincidunt neque augue lacinia . Aex conubia ex vivamus ligula proin sit . Bibendum habitasse cursus congue morbi platea . Taciti est finibus accumsan iaculis montes rutrum . Consequat vulputate odio conubia sociosqu imperdiet porta ! [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_optimizer import get_supported_optimizers\n",
        "\n",
        "supported_optimizers = get_supported_optimizers()\n",
        "\n",
        "print(supported_optimizers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5qWuNYM2xPi",
        "outputId": "50db9496-a3be-4653-c3a6-ef77a1f75c0d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a2grad', 'accsgd', 'adabelief', 'adabound', 'adadelta', 'adafactor', 'adahessian', 'adai', 'adalite', 'adalomo', 'adamax', 'adamg', 'adammini', 'adamod', 'adamp', 'adams', 'adamw', 'adan', 'adanorm', 'adapnm', 'adashift', 'adasmooth', 'ademamix', 'adopt', 'aggmo', 'aida', 'alig', 'amos', 'apollo', 'asgd', 'avagrad', 'bsam', 'came', 'dadaptadagrad', 'dadaptadam', 'dadaptadan', 'dadaptlion', 'dadaptsgd', 'diffgrad', 'fadam', 'fromage', 'ftrl', 'galore', 'gravity', 'grokfastadamw', 'kate', 'lamb', 'lars', 'lion', 'lomo', 'madgrad', 'msvag', 'nero', 'novograd', 'padam', 'pid', 'pnm', 'prodigy', 'qhadam', 'qhm', 'radam', 'ranger', 'ranger21', 'scalableshampoo', 'schedulefreeadamw', 'schedulefreesgd', 'sgdp', 'sgdw', 'shampoo', 'signsgd', 'sm3', 'soap', 'sophiah', 'srmm', 'stableadamw', 'swats', 'tiger', 'yogi']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_optimizer import create_optimizer\n",
        "\n",
        "# Training Loop\n",
        "model = TransformerModel(num_layers=8, dim=128, num_heads=8, num_groups=4, max_seq_len=seq_len, vocab_size=tokenizer.get_vocab_size(), device='cuda')\n",
        "model.to('cuda')\n",
        "\n",
        "optimizer = create_optimizer(\n",
        "    model,\n",
        "    'soap',\n",
        "    lr=3e-3,\n",
        "    weight_decay=0.01,\n",
        "    betas=(.95, .95),\n",
        "    precondition_frequency=10\n",
        ")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.token_to_id(\"[PAD]\"))\n",
        "\n",
        "# In the training loop, ensure inputs and targets are on the correct device\n",
        "for epoch in range(1000):\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        input_batch, target_batch = batch\n",
        "        input_batch = input_batch.to('cuda', dtype=torch.long)\n",
        "        target_batch = target_batch.to('cuda', dtype=torch.long)\n",
        "        logits = model(input_batch)\n",
        "        loss = loss_fn(logits.view(-1, tokenizer.get_vocab_size()), target_batch.view(-1))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(dataloader)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LL8UGHiwmZBe",
        "outputId": "c8d89b30-8e6e-4e74-c695-834485de4b12"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 6.702381610870361\n",
            "Epoch 2, Loss: 6.7023820877075195\n",
            "Epoch 3, Loss: 6.682211875915527\n",
            "Epoch 4, Loss: 6.639337062835693\n",
            "Epoch 5, Loss: 6.642791271209717\n",
            "Epoch 6, Loss: 6.620864391326904\n",
            "Epoch 7, Loss: 6.606772422790527\n",
            "Epoch 8, Loss: 6.598873138427734\n",
            "Epoch 9, Loss: 6.5926618576049805\n",
            "Epoch 10, Loss: 6.5850419998168945\n",
            "Epoch 11, Loss: 6.57772970199585\n",
            "Epoch 12, Loss: 6.572871685028076\n",
            "Epoch 13, Loss: 6.568054676055908\n",
            "Epoch 14, Loss: 6.563277244567871\n",
            "Epoch 15, Loss: 6.55855655670166\n",
            "Epoch 16, Loss: 6.553890228271484\n",
            "Epoch 17, Loss: 6.549271583557129\n",
            "Epoch 18, Loss: 6.544683933258057\n",
            "Epoch 19, Loss: 6.540124416351318\n",
            "Epoch 20, Loss: 6.5355682373046875\n",
            "Epoch 21, Loss: 6.530980587005615\n",
            "Epoch 22, Loss: 6.526450157165527\n",
            "Epoch 23, Loss: 6.521883010864258\n",
            "Epoch 24, Loss: 6.517276287078857\n",
            "Epoch 25, Loss: 6.512630939483643\n",
            "Epoch 26, Loss: 6.507955551147461\n",
            "Epoch 27, Loss: 6.503248691558838\n",
            "Epoch 28, Loss: 6.498510837554932\n",
            "Epoch 29, Loss: 6.493733882904053\n",
            "Epoch 30, Loss: 6.4888997077941895\n",
            "Epoch 31, Loss: 6.48399019241333\n",
            "Epoch 32, Loss: 6.479203224182129\n",
            "Epoch 33, Loss: 6.474367141723633\n",
            "Epoch 34, Loss: 6.469454765319824\n",
            "Epoch 35, Loss: 6.46443510055542\n",
            "Epoch 36, Loss: 6.459261417388916\n",
            "Epoch 37, Loss: 6.453874588012695\n",
            "Epoch 38, Loss: 6.448179244995117\n",
            "Epoch 39, Loss: 6.442023754119873\n",
            "Epoch 40, Loss: 6.435169219970703\n",
            "Epoch 41, Loss: 6.427254676818848\n",
            "Epoch 42, Loss: 6.420384883880615\n",
            "Epoch 43, Loss: 6.412954807281494\n",
            "Epoch 44, Loss: 6.404621124267578\n",
            "Epoch 45, Loss: 6.395115852355957\n",
            "Epoch 46, Loss: 6.38426399230957\n",
            "Epoch 47, Loss: 6.371303081512451\n",
            "Epoch 48, Loss: 6.355813980102539\n",
            "Epoch 49, Loss: 6.3374409675598145\n",
            "Epoch 50, Loss: 6.316944599151611\n",
            "Epoch 51, Loss: 6.313216686248779\n",
            "Epoch 52, Loss: 6.286285877227783\n",
            "Epoch 53, Loss: 6.282481670379639\n",
            "Epoch 54, Loss: 6.277523994445801\n",
            "Epoch 55, Loss: 6.25880765914917\n",
            "Epoch 56, Loss: 6.244710445404053\n",
            "Epoch 57, Loss: 6.239700794219971\n",
            "Epoch 58, Loss: 6.225797176361084\n",
            "Epoch 59, Loss: 6.2067952156066895\n",
            "Epoch 60, Loss: 6.193645477294922\n",
            "Epoch 61, Loss: 6.181555271148682\n",
            "Epoch 62, Loss: 6.168262481689453\n",
            "Epoch 63, Loss: 6.154277324676514\n",
            "Epoch 64, Loss: 6.1421990394592285\n",
            "Epoch 65, Loss: 6.1311421394348145\n",
            "Epoch 66, Loss: 6.1181488037109375\n",
            "Epoch 67, Loss: 6.103341102600098\n",
            "Epoch 68, Loss: 6.08881139755249\n",
            "Epoch 69, Loss: 6.074662685394287\n",
            "Epoch 70, Loss: 6.060104846954346\n",
            "Epoch 71, Loss: 6.044867038726807\n",
            "Epoch 72, Loss: 6.031721591949463\n",
            "Epoch 73, Loss: 6.018029689788818\n",
            "Epoch 74, Loss: 6.004490852355957\n",
            "Epoch 75, Loss: 5.991155624389648\n",
            "Epoch 76, Loss: 5.978376388549805\n",
            "Epoch 77, Loss: 5.965173721313477\n",
            "Epoch 78, Loss: 5.95102071762085\n",
            "Epoch 79, Loss: 5.936805725097656\n",
            "Epoch 80, Loss: 5.920382499694824\n",
            "Epoch 81, Loss: 5.904208660125732\n",
            "Epoch 82, Loss: 5.889861583709717\n",
            "Epoch 83, Loss: 5.876216888427734\n",
            "Epoch 84, Loss: 5.861546516418457\n",
            "Epoch 85, Loss: 5.8475470542907715\n",
            "Epoch 86, Loss: 5.833091735839844\n",
            "Epoch 87, Loss: 5.818265438079834\n",
            "Epoch 88, Loss: 5.802724838256836\n",
            "Epoch 89, Loss: 5.7875800132751465\n",
            "Epoch 90, Loss: 5.774350166320801\n",
            "Epoch 91, Loss: 5.759570598602295\n",
            "Epoch 92, Loss: 5.741666316986084\n",
            "Epoch 93, Loss: 5.72918701171875\n",
            "Epoch 94, Loss: 5.712895393371582\n",
            "Epoch 95, Loss: 5.698972702026367\n",
            "Epoch 96, Loss: 5.683503150939941\n",
            "Epoch 97, Loss: 5.668161869049072\n",
            "Epoch 98, Loss: 5.6521735191345215\n",
            "Epoch 99, Loss: 5.637259483337402\n",
            "Epoch 100, Loss: 5.626453399658203\n",
            "Epoch 101, Loss: 5.613072395324707\n",
            "Epoch 102, Loss: 5.595686912536621\n",
            "Epoch 103, Loss: 5.576598644256592\n",
            "Epoch 104, Loss: 5.563058376312256\n",
            "Epoch 105, Loss: 5.545608043670654\n",
            "Epoch 106, Loss: 5.528568744659424\n",
            "Epoch 107, Loss: 5.512301445007324\n",
            "Epoch 108, Loss: 5.494768142700195\n",
            "Epoch 109, Loss: 5.476527690887451\n",
            "Epoch 110, Loss: 5.458600044250488\n",
            "Epoch 111, Loss: 5.441762447357178\n",
            "Epoch 112, Loss: 5.428889751434326\n",
            "Epoch 113, Loss: 5.41099214553833\n",
            "Epoch 114, Loss: 5.395352363586426\n",
            "Epoch 115, Loss: 5.373867988586426\n",
            "Epoch 116, Loss: 5.359266757965088\n",
            "Epoch 117, Loss: 5.338137149810791\n",
            "Epoch 118, Loss: 5.318703651428223\n",
            "Epoch 119, Loss: 5.298083305358887\n",
            "Epoch 120, Loss: 5.278902530670166\n",
            "Epoch 121, Loss: 5.25942325592041\n",
            "Epoch 122, Loss: 5.23741340637207\n",
            "Epoch 123, Loss: 5.220594882965088\n",
            "Epoch 124, Loss: 5.2057929039001465\n",
            "Epoch 125, Loss: 5.187742710113525\n",
            "Epoch 126, Loss: 5.169642925262451\n",
            "Epoch 127, Loss: 5.146184921264648\n",
            "Epoch 128, Loss: 5.122029781341553\n",
            "Epoch 129, Loss: 5.100170612335205\n",
            "Epoch 130, Loss: 5.07598876953125\n",
            "Epoch 131, Loss: 5.055181980133057\n",
            "Epoch 132, Loss: 5.031559944152832\n",
            "Epoch 133, Loss: 5.008783340454102\n",
            "Epoch 134, Loss: 4.986112117767334\n",
            "Epoch 135, Loss: 4.9639811515808105\n",
            "Epoch 136, Loss: 4.944334030151367\n",
            "Epoch 137, Loss: 4.92796516418457\n",
            "Epoch 138, Loss: 4.897855758666992\n",
            "Epoch 139, Loss: 4.872805118560791\n",
            "Epoch 140, Loss: 4.843354225158691\n",
            "Epoch 141, Loss: 4.822040557861328\n",
            "Epoch 142, Loss: 4.794551372528076\n",
            "Epoch 143, Loss: 4.765142440795898\n",
            "Epoch 144, Loss: 4.7407002449035645\n",
            "Epoch 145, Loss: 4.714369297027588\n",
            "Epoch 146, Loss: 4.686662197113037\n",
            "Epoch 147, Loss: 4.658053874969482\n",
            "Epoch 148, Loss: 4.629143238067627\n",
            "Epoch 149, Loss: 4.601592540740967\n",
            "Epoch 150, Loss: 4.576199531555176\n",
            "Epoch 151, Loss: 4.5497212409973145\n",
            "Epoch 152, Loss: 4.513452529907227\n",
            "Epoch 153, Loss: 4.48906135559082\n",
            "Epoch 154, Loss: 4.465933322906494\n",
            "Epoch 155, Loss: 4.429800510406494\n",
            "Epoch 156, Loss: 4.403525352478027\n",
            "Epoch 157, Loss: 4.36979341506958\n",
            "Epoch 158, Loss: 4.336711406707764\n",
            "Epoch 159, Loss: 4.3032050132751465\n",
            "Epoch 160, Loss: 4.267932415008545\n",
            "Epoch 161, Loss: 4.231357097625732\n",
            "Epoch 162, Loss: 4.19765043258667\n",
            "Epoch 163, Loss: 4.162527084350586\n",
            "Epoch 164, Loss: 4.1290788650512695\n",
            "Epoch 165, Loss: 4.093975067138672\n",
            "Epoch 166, Loss: 4.057881832122803\n",
            "Epoch 167, Loss: 4.0209808349609375\n",
            "Epoch 168, Loss: 3.982231378555298\n",
            "Epoch 169, Loss: 3.943547010421753\n",
            "Epoch 170, Loss: 3.903764486312866\n",
            "Epoch 171, Loss: 3.8624300956726074\n",
            "Epoch 172, Loss: 3.825241804122925\n",
            "Epoch 173, Loss: 3.7914857864379883\n",
            "Epoch 174, Loss: 3.7667574882507324\n",
            "Epoch 175, Loss: 3.7265026569366455\n",
            "Epoch 176, Loss: 3.690434217453003\n",
            "Epoch 177, Loss: 3.6519625186920166\n",
            "Epoch 178, Loss: 3.6195199489593506\n",
            "Epoch 179, Loss: 3.579253673553467\n",
            "Epoch 180, Loss: 3.5393638610839844\n",
            "Epoch 181, Loss: 3.5021324157714844\n",
            "Epoch 182, Loss: 3.4599595069885254\n",
            "Epoch 183, Loss: 3.4250309467315674\n",
            "Epoch 184, Loss: 3.3895092010498047\n",
            "Epoch 185, Loss: 3.355682611465454\n",
            "Epoch 186, Loss: 3.3187077045440674\n",
            "Epoch 187, Loss: 3.283433437347412\n",
            "Epoch 188, Loss: 3.246631145477295\n",
            "Epoch 189, Loss: 3.2103824615478516\n",
            "Epoch 190, Loss: 3.1770050525665283\n",
            "Epoch 191, Loss: 3.148224115371704\n",
            "Epoch 192, Loss: 3.1090071201324463\n",
            "Epoch 193, Loss: 3.0801913738250732\n",
            "Epoch 194, Loss: 3.045043468475342\n",
            "Epoch 195, Loss: 3.015843391418457\n",
            "Epoch 196, Loss: 2.9830081462860107\n",
            "Epoch 197, Loss: 2.9510693550109863\n",
            "Epoch 198, Loss: 2.917656421661377\n",
            "Epoch 199, Loss: 2.886786937713623\n",
            "Epoch 200, Loss: 2.853814125061035\n",
            "Epoch 201, Loss: 2.825782537460327\n",
            "Epoch 202, Loss: 2.7935755252838135\n",
            "Epoch 203, Loss: 2.7630622386932373\n",
            "Epoch 204, Loss: 2.7364935874938965\n",
            "Epoch 205, Loss: 2.7081942558288574\n",
            "Epoch 206, Loss: 2.6794352531433105\n",
            "Epoch 207, Loss: 2.6526308059692383\n",
            "Epoch 208, Loss: 2.6259453296661377\n",
            "Epoch 209, Loss: 2.602278470993042\n",
            "Epoch 210, Loss: 2.5762135982513428\n",
            "Epoch 211, Loss: 2.551218271255493\n",
            "Epoch 212, Loss: 2.519326686859131\n",
            "Epoch 213, Loss: 2.496737480163574\n",
            "Epoch 214, Loss: 2.4694759845733643\n",
            "Epoch 215, Loss: 2.4455504417419434\n",
            "Epoch 216, Loss: 2.419008255004883\n",
            "Epoch 217, Loss: 2.3939898014068604\n",
            "Epoch 218, Loss: 2.367509365081787\n",
            "Epoch 219, Loss: 2.34432053565979\n",
            "Epoch 220, Loss: 2.3209774494171143\n",
            "Epoch 221, Loss: 2.298696517944336\n",
            "Epoch 222, Loss: 2.2714569568634033\n",
            "Epoch 223, Loss: 2.2496652603149414\n",
            "Epoch 224, Loss: 2.226680040359497\n",
            "Epoch 225, Loss: 2.2034945487976074\n",
            "Epoch 226, Loss: 2.1798036098480225\n",
            "Epoch 227, Loss: 2.1560938358306885\n",
            "Epoch 228, Loss: 2.1339540481567383\n",
            "Epoch 229, Loss: 2.113734245300293\n",
            "Epoch 230, Loss: 2.0975589752197266\n",
            "Epoch 231, Loss: 2.069974660873413\n",
            "Epoch 232, Loss: 2.046931743621826\n",
            "Epoch 233, Loss: 2.023683547973633\n",
            "Epoch 234, Loss: 2.005359411239624\n",
            "Epoch 235, Loss: 1.980689525604248\n",
            "Epoch 236, Loss: 1.9595293998718262\n",
            "Epoch 237, Loss: 1.9363166093826294\n",
            "Epoch 238, Loss: 1.9154551029205322\n",
            "Epoch 239, Loss: 1.8950172662734985\n",
            "Epoch 240, Loss: 1.8769892454147339\n",
            "Epoch 241, Loss: 1.854110598564148\n",
            "Epoch 242, Loss: 1.8352328538894653\n",
            "Epoch 243, Loss: 1.8134673833847046\n",
            "Epoch 244, Loss: 1.7921912670135498\n",
            "Epoch 245, Loss: 1.7717782258987427\n",
            "Epoch 246, Loss: 1.7541793584823608\n",
            "Epoch 247, Loss: 1.7338292598724365\n",
            "Epoch 248, Loss: 1.7123353481292725\n",
            "Epoch 249, Loss: 1.693189263343811\n",
            "Epoch 250, Loss: 1.6703815460205078\n",
            "Epoch 251, Loss: 1.6509193181991577\n",
            "Epoch 252, Loss: 1.6298248767852783\n",
            "Epoch 253, Loss: 1.6102577447891235\n",
            "Epoch 254, Loss: 1.5927228927612305\n",
            "Epoch 255, Loss: 1.5772448778152466\n",
            "Epoch 256, Loss: 1.5562185049057007\n",
            "Epoch 257, Loss: 1.534501075744629\n",
            "Epoch 258, Loss: 1.5181641578674316\n",
            "Epoch 259, Loss: 1.499835729598999\n",
            "Epoch 260, Loss: 1.4795424938201904\n",
            "Epoch 261, Loss: 1.4620914459228516\n",
            "Epoch 262, Loss: 1.4447921514511108\n",
            "Epoch 263, Loss: 1.4275054931640625\n",
            "Epoch 264, Loss: 1.4084666967391968\n",
            "Epoch 265, Loss: 1.3903332948684692\n",
            "Epoch 266, Loss: 1.3721784353256226\n",
            "Epoch 267, Loss: 1.3554126024246216\n",
            "Epoch 268, Loss: 1.3383103609085083\n",
            "Epoch 269, Loss: 1.3218894004821777\n",
            "Epoch 270, Loss: 1.3054945468902588\n",
            "Epoch 271, Loss: 1.2883378267288208\n",
            "Epoch 272, Loss: 1.2707771062850952\n",
            "Epoch 273, Loss: 1.255418062210083\n",
            "Epoch 274, Loss: 1.2398439645767212\n",
            "Epoch 275, Loss: 1.2254709005355835\n",
            "Epoch 276, Loss: 1.2116472721099854\n",
            "Epoch 277, Loss: 1.1993975639343262\n",
            "Epoch 278, Loss: 1.1835016012191772\n",
            "Epoch 279, Loss: 1.1684017181396484\n",
            "Epoch 280, Loss: 1.1528874635696411\n",
            "Epoch 281, Loss: 1.1377373933792114\n",
            "Epoch 282, Loss: 1.1218748092651367\n",
            "Epoch 283, Loss: 1.1072921752929688\n",
            "Epoch 284, Loss: 1.0946948528289795\n",
            "Epoch 285, Loss: 1.08074951171875\n",
            "Epoch 286, Loss: 1.0649462938308716\n",
            "Epoch 287, Loss: 1.051903247833252\n",
            "Epoch 288, Loss: 1.0378336906433105\n",
            "Epoch 289, Loss: 1.0238640308380127\n",
            "Epoch 290, Loss: 1.0101619958877563\n",
            "Epoch 291, Loss: 0.9965590238571167\n",
            "Epoch 292, Loss: 0.983475387096405\n",
            "Epoch 293, Loss: 0.9705356359481812\n",
            "Epoch 294, Loss: 0.9580815434455872\n",
            "Epoch 295, Loss: 0.9458087086677551\n",
            "Epoch 296, Loss: 0.9335941076278687\n",
            "Epoch 297, Loss: 0.9217246174812317\n",
            "Epoch 298, Loss: 0.909827709197998\n",
            "Epoch 299, Loss: 0.8992258310317993\n",
            "Epoch 300, Loss: 0.8878517150878906\n",
            "Epoch 301, Loss: 0.8737524151802063\n",
            "Epoch 302, Loss: 0.8629226088523865\n",
            "Epoch 303, Loss: 0.8512177467346191\n",
            "Epoch 304, Loss: 0.840954065322876\n",
            "Epoch 305, Loss: 0.8299121856689453\n",
            "Epoch 306, Loss: 0.8191602230072021\n",
            "Epoch 307, Loss: 0.8090822696685791\n",
            "Epoch 308, Loss: 0.7988643646240234\n",
            "Epoch 309, Loss: 0.7891040444374084\n",
            "Epoch 310, Loss: 0.7789294123649597\n",
            "Epoch 311, Loss: 0.768258273601532\n",
            "Epoch 312, Loss: 0.7573747038841248\n",
            "Epoch 313, Loss: 0.7470095157623291\n",
            "Epoch 314, Loss: 0.7366093397140503\n",
            "Epoch 315, Loss: 0.7268543839454651\n",
            "Epoch 316, Loss: 0.7171871066093445\n",
            "Epoch 317, Loss: 0.7069575190544128\n",
            "Epoch 318, Loss: 0.6974859833717346\n",
            "Epoch 319, Loss: 0.6877307295799255\n",
            "Epoch 320, Loss: 0.6774139404296875\n",
            "Epoch 321, Loss: 0.6679133772850037\n",
            "Epoch 322, Loss: 0.6580824255943298\n",
            "Epoch 323, Loss: 0.6493923664093018\n",
            "Epoch 324, Loss: 0.6405644416809082\n",
            "Epoch 325, Loss: 0.631496250629425\n",
            "Epoch 326, Loss: 0.6227466464042664\n",
            "Epoch 327, Loss: 0.6142059564590454\n",
            "Epoch 328, Loss: 0.6054100394248962\n",
            "Epoch 329, Loss: 0.5966352820396423\n",
            "Epoch 330, Loss: 0.5881696343421936\n",
            "Epoch 331, Loss: 0.5797238945960999\n",
            "Epoch 332, Loss: 0.5715128183364868\n",
            "Epoch 333, Loss: 0.5641039609909058\n",
            "Epoch 334, Loss: 0.5576567053794861\n",
            "Epoch 335, Loss: 0.5503730773925781\n",
            "Epoch 336, Loss: 0.5445954203605652\n",
            "Epoch 337, Loss: 0.5359935164451599\n",
            "Epoch 338, Loss: 0.5278662443161011\n",
            "Epoch 339, Loss: 0.5185152292251587\n",
            "Epoch 340, Loss: 0.5112195014953613\n",
            "Epoch 341, Loss: 0.5026556253433228\n",
            "Epoch 342, Loss: 0.49462535977363586\n",
            "Epoch 343, Loss: 0.4869813919067383\n",
            "Epoch 344, Loss: 0.47943779826164246\n",
            "Epoch 345, Loss: 0.471630722284317\n",
            "Epoch 346, Loss: 0.4646415114402771\n",
            "Epoch 347, Loss: 0.45742419362068176\n",
            "Epoch 348, Loss: 0.45022305846214294\n",
            "Epoch 349, Loss: 0.44300344586372375\n",
            "Epoch 350, Loss: 0.43576377630233765\n",
            "Epoch 351, Loss: 0.4286416471004486\n",
            "Epoch 352, Loss: 0.4216962456703186\n",
            "Epoch 353, Loss: 0.41447341442108154\n",
            "Epoch 354, Loss: 0.40769967436790466\n",
            "Epoch 355, Loss: 0.40105438232421875\n",
            "Epoch 356, Loss: 0.3941856920719147\n",
            "Epoch 357, Loss: 0.3873208463191986\n",
            "Epoch 358, Loss: 0.3802606761455536\n",
            "Epoch 359, Loss: 0.3741249442100525\n",
            "Epoch 360, Loss: 0.36718302965164185\n",
            "Epoch 361, Loss: 0.3606839179992676\n",
            "Epoch 362, Loss: 0.3542088270187378\n",
            "Epoch 363, Loss: 0.34804683923721313\n",
            "Epoch 364, Loss: 0.3421047627925873\n",
            "Epoch 365, Loss: 0.3363494575023651\n",
            "Epoch 366, Loss: 0.33083295822143555\n",
            "Epoch 367, Loss: 0.3253503441810608\n",
            "Epoch 368, Loss: 0.3195441961288452\n",
            "Epoch 369, Loss: 0.31356894969940186\n",
            "Epoch 370, Loss: 0.30832940340042114\n",
            "Epoch 371, Loss: 0.30282536149024963\n",
            "Epoch 372, Loss: 0.29737240076065063\n",
            "Epoch 373, Loss: 0.2924828827381134\n",
            "Epoch 374, Loss: 0.28652554750442505\n",
            "Epoch 375, Loss: 0.28197014331817627\n",
            "Epoch 376, Loss: 0.2764931321144104\n",
            "Epoch 377, Loss: 0.27167773246765137\n",
            "Epoch 378, Loss: 0.26616358757019043\n",
            "Epoch 379, Loss: 0.26140275597572327\n",
            "Epoch 380, Loss: 0.2562926709651947\n",
            "Epoch 381, Loss: 0.2514322102069855\n",
            "Epoch 382, Loss: 0.24677811563014984\n",
            "Epoch 383, Loss: 0.24206341803073883\n",
            "Epoch 384, Loss: 0.23766741156578064\n",
            "Epoch 385, Loss: 0.23317843675613403\n",
            "Epoch 386, Loss: 0.228773295879364\n",
            "Epoch 387, Loss: 0.22453561425209045\n",
            "Epoch 388, Loss: 0.22011253237724304\n",
            "Epoch 389, Loss: 0.2157745212316513\n",
            "Epoch 390, Loss: 0.2111871987581253\n",
            "Epoch 391, Loss: 0.20721620321273804\n",
            "Epoch 392, Loss: 0.20405042171478271\n",
            "Epoch 393, Loss: 0.20107623934745789\n",
            "Epoch 394, Loss: 0.19735677540302277\n",
            "Epoch 395, Loss: 0.19361752271652222\n",
            "Epoch 396, Loss: 0.1899428367614746\n",
            "Epoch 397, Loss: 0.18631801009178162\n",
            "Epoch 398, Loss: 0.18271681666374207\n",
            "Epoch 399, Loss: 0.1791691780090332\n",
            "Epoch 400, Loss: 0.17567695677280426\n",
            "Epoch 401, Loss: 0.172178715467453\n",
            "Epoch 402, Loss: 0.1687757670879364\n",
            "Epoch 403, Loss: 0.1655511111021042\n",
            "Epoch 404, Loss: 0.16233132779598236\n",
            "Epoch 405, Loss: 0.15923073887825012\n",
            "Epoch 406, Loss: 0.15617181360721588\n",
            "Epoch 407, Loss: 0.15304365754127502\n",
            "Epoch 408, Loss: 0.1500493288040161\n",
            "Epoch 409, Loss: 0.14710107445716858\n",
            "Epoch 410, Loss: 0.1441485583782196\n",
            "Epoch 411, Loss: 0.14173676073551178\n",
            "Epoch 412, Loss: 0.13861550390720367\n",
            "Epoch 413, Loss: 0.13583657145500183\n",
            "Epoch 414, Loss: 0.13347971439361572\n",
            "Epoch 415, Loss: 0.13055987656116486\n",
            "Epoch 416, Loss: 0.12814857065677643\n",
            "Epoch 417, Loss: 0.12566730380058289\n",
            "Epoch 418, Loss: 0.12669113278388977\n",
            "Epoch 419, Loss: 0.14675447344779968\n",
            "Epoch 420, Loss: 0.1410728096961975\n",
            "Epoch 421, Loss: 0.12684573233127594\n",
            "Epoch 422, Loss: 0.12235154956579208\n",
            "Epoch 423, Loss: 0.11885520070791245\n",
            "Epoch 424, Loss: 0.11568545550107956\n",
            "Epoch 425, Loss: 0.11425091326236725\n",
            "Epoch 426, Loss: 0.111813023686409\n",
            "Epoch 427, Loss: 0.10960783809423447\n",
            "Epoch 428, Loss: 0.10732405632734299\n",
            "Epoch 429, Loss: 0.10517212003469467\n",
            "Epoch 430, Loss: 0.10295696556568146\n",
            "Epoch 431, Loss: 0.10076174885034561\n",
            "Epoch 432, Loss: 0.09859912097454071\n",
            "Epoch 433, Loss: 0.09626566618680954\n",
            "Epoch 434, Loss: 0.09396862983703613\n",
            "Epoch 435, Loss: 0.09201502799987793\n",
            "Epoch 436, Loss: 0.09011920541524887\n",
            "Epoch 437, Loss: 0.08824589848518372\n",
            "Epoch 438, Loss: 0.0864097848534584\n",
            "Epoch 439, Loss: 0.0846133902668953\n",
            "Epoch 440, Loss: 0.0828237310051918\n",
            "Epoch 441, Loss: 0.08105826377868652\n",
            "Epoch 442, Loss: 0.07937850058078766\n",
            "Epoch 443, Loss: 0.07770808041095734\n",
            "Epoch 444, Loss: 0.07608439028263092\n",
            "Epoch 445, Loss: 0.07449817657470703\n",
            "Epoch 446, Loss: 0.07297524809837341\n",
            "Epoch 447, Loss: 0.07149381190538406\n",
            "Epoch 448, Loss: 0.07004213333129883\n",
            "Epoch 449, Loss: 0.06862856447696686\n",
            "Epoch 450, Loss: 0.0672370120882988\n",
            "Epoch 451, Loss: 0.06586165726184845\n",
            "Epoch 452, Loss: 0.06452614068984985\n",
            "Epoch 453, Loss: 0.06323040276765823\n",
            "Epoch 454, Loss: 0.06197688356041908\n",
            "Epoch 455, Loss: 0.06075676903128624\n",
            "Epoch 456, Loss: 0.05958505719900131\n",
            "Epoch 457, Loss: 0.058434758335351944\n",
            "Epoch 458, Loss: 0.05728832259774208\n",
            "Epoch 459, Loss: 0.056176673620939255\n",
            "Epoch 460, Loss: 0.05509660020470619\n",
            "Epoch 461, Loss: 0.053999193012714386\n",
            "Epoch 462, Loss: 0.052965275943279266\n",
            "Epoch 463, Loss: 0.051954131573438644\n",
            "Epoch 464, Loss: 0.050999414175748825\n",
            "Epoch 465, Loss: 0.05002070218324661\n",
            "Epoch 466, Loss: 0.04908280447125435\n",
            "Epoch 467, Loss: 0.04817056283354759\n",
            "Epoch 468, Loss: 0.04726758599281311\n",
            "Epoch 469, Loss: 0.04639746621251106\n",
            "Epoch 470, Loss: 0.04555065557360649\n",
            "Epoch 471, Loss: 0.04472411796450615\n",
            "Epoch 472, Loss: 0.04391724616289139\n",
            "Epoch 473, Loss: 0.04308956861495972\n",
            "Epoch 474, Loss: 0.042320456355810165\n",
            "Epoch 475, Loss: 0.041575971990823746\n",
            "Epoch 476, Loss: 0.040799450129270554\n",
            "Epoch 477, Loss: 0.04009329155087471\n",
            "Epoch 478, Loss: 0.03936462104320526\n",
            "Epoch 479, Loss: 0.038668252527713776\n",
            "Epoch 480, Loss: 0.0379890576004982\n",
            "Epoch 481, Loss: 0.037306610494852066\n",
            "Epoch 482, Loss: 0.036653343588113785\n",
            "Epoch 483, Loss: 0.03601904958486557\n",
            "Epoch 484, Loss: 0.03539729490876198\n",
            "Epoch 485, Loss: 0.03479341045022011\n",
            "Epoch 486, Loss: 0.034219857305288315\n",
            "Epoch 487, Loss: 0.03365487977862358\n",
            "Epoch 488, Loss: 0.033076975494623184\n",
            "Epoch 489, Loss: 0.03248364478349686\n",
            "Epoch 490, Loss: 0.03192904591560364\n",
            "Epoch 491, Loss: 0.03139573708176613\n",
            "Epoch 492, Loss: 0.030842158943414688\n",
            "Epoch 493, Loss: 0.030340446159243584\n",
            "Epoch 494, Loss: 0.029826372861862183\n",
            "Epoch 495, Loss: 0.029329590499401093\n",
            "Epoch 496, Loss: 0.02884337492287159\n",
            "Epoch 497, Loss: 0.028361521661281586\n",
            "Epoch 498, Loss: 0.02789619192481041\n",
            "Epoch 499, Loss: 0.027438422664999962\n",
            "Epoch 500, Loss: 0.026977447792887688\n",
            "Epoch 501, Loss: 0.026543736457824707\n",
            "Epoch 502, Loss: 0.02610684372484684\n",
            "Epoch 503, Loss: 0.025680774822831154\n",
            "Epoch 504, Loss: 0.025269754230976105\n",
            "Epoch 505, Loss: 0.02486751601099968\n",
            "Epoch 506, Loss: 0.024467570707201958\n",
            "Epoch 507, Loss: 0.02407209202647209\n",
            "Epoch 508, Loss: 0.02369247004389763\n",
            "Epoch 509, Loss: 0.023319320753216743\n",
            "Epoch 510, Loss: 0.022946901619434357\n",
            "Epoch 511, Loss: 0.02258460968732834\n",
            "Epoch 512, Loss: 0.022233784198760986\n",
            "Epoch 513, Loss: 0.02188270352780819\n",
            "Epoch 514, Loss: 0.021543601527810097\n",
            "Epoch 515, Loss: 0.02121105045080185\n",
            "Epoch 516, Loss: 0.020882882177829742\n",
            "Epoch 517, Loss: 0.020563824102282524\n",
            "Epoch 518, Loss: 0.02024572715163231\n",
            "Epoch 519, Loss: 0.01993604749441147\n",
            "Epoch 520, Loss: 0.019630899652838707\n",
            "Epoch 521, Loss: 0.019332820549607277\n",
            "Epoch 522, Loss: 0.019039258360862732\n",
            "Epoch 523, Loss: 0.018756259232759476\n",
            "Epoch 524, Loss: 0.018474604934453964\n",
            "Epoch 525, Loss: 0.018204789608716965\n",
            "Epoch 526, Loss: 0.017933307215571404\n",
            "Epoch 527, Loss: 0.017665604129433632\n",
            "Epoch 528, Loss: 0.017410272732377052\n",
            "Epoch 529, Loss: 0.017152972519397736\n",
            "Epoch 530, Loss: 0.016904469579458237\n",
            "Epoch 531, Loss: 0.01665673404932022\n",
            "Epoch 532, Loss: 0.016415810212492943\n",
            "Epoch 533, Loss: 0.016179809346795082\n",
            "Epoch 534, Loss: 0.015952792018651962\n",
            "Epoch 535, Loss: 0.01572428084909916\n",
            "Epoch 536, Loss: 0.015502235852181911\n",
            "Epoch 537, Loss: 0.015288452617824078\n",
            "Epoch 538, Loss: 0.015070143155753613\n",
            "Epoch 539, Loss: 0.014861609786748886\n",
            "Epoch 540, Loss: 0.014660581946372986\n",
            "Epoch 541, Loss: 0.014471504837274551\n",
            "Epoch 542, Loss: 0.014256921596825123\n",
            "Epoch 543, Loss: 0.014073261991143227\n",
            "Epoch 544, Loss: 0.01387266255915165\n",
            "Epoch 545, Loss: 0.01369685772806406\n",
            "Epoch 546, Loss: 0.013502124696969986\n",
            "Epoch 547, Loss: 0.013332841917872429\n",
            "Epoch 548, Loss: 0.013151725754141808\n",
            "Epoch 549, Loss: 0.012982916086912155\n",
            "Epoch 550, Loss: 0.012814772315323353\n",
            "Epoch 551, Loss: 0.012641701847314835\n",
            "Epoch 552, Loss: 0.012477409094572067\n",
            "Epoch 553, Loss: 0.01232230570167303\n",
            "Epoch 554, Loss: 0.012160769663751125\n",
            "Epoch 555, Loss: 0.01200909074395895\n",
            "Epoch 556, Loss: 0.011857744306325912\n",
            "Epoch 557, Loss: 0.011712058447301388\n",
            "Epoch 558, Loss: 0.011565346270799637\n",
            "Epoch 559, Loss: 0.011420905590057373\n",
            "Epoch 560, Loss: 0.011282452382147312\n",
            "Epoch 561, Loss: 0.011142579838633537\n",
            "Epoch 562, Loss: 0.011008729226887226\n",
            "Epoch 563, Loss: 0.010879257693886757\n",
            "Epoch 564, Loss: 0.0107510294765234\n",
            "Epoch 565, Loss: 0.010629276745021343\n",
            "Epoch 566, Loss: 0.010505239479243755\n",
            "Epoch 567, Loss: 0.010380346328020096\n",
            "Epoch 568, Loss: 0.010260294191539288\n",
            "Epoch 569, Loss: 0.010141139850020409\n",
            "Epoch 570, Loss: 0.010028237476944923\n",
            "Epoch 571, Loss: 0.009910359978675842\n",
            "Epoch 572, Loss: 0.009799756109714508\n",
            "Epoch 573, Loss: 0.009693551808595657\n",
            "Epoch 574, Loss: 0.009586140513420105\n",
            "Epoch 575, Loss: 0.009482070803642273\n",
            "Epoch 576, Loss: 0.009380660951137543\n",
            "Epoch 577, Loss: 0.009279422461986542\n",
            "Epoch 578, Loss: 0.009181699715554714\n",
            "Epoch 579, Loss: 0.009083974175155163\n",
            "Epoch 580, Loss: 0.008988676592707634\n",
            "Epoch 581, Loss: 0.008897143416106701\n",
            "Epoch 582, Loss: 0.008805734105408192\n",
            "Epoch 583, Loss: 0.008717702701687813\n",
            "Epoch 584, Loss: 0.008632509969174862\n",
            "Epoch 585, Loss: 0.008551371283829212\n",
            "Epoch 586, Loss: 0.00846367422491312\n",
            "Epoch 587, Loss: 0.008376727811992168\n",
            "Epoch 588, Loss: 0.008297407999634743\n",
            "Epoch 589, Loss: 0.00821958389133215\n",
            "Epoch 590, Loss: 0.008142889477312565\n",
            "Epoch 591, Loss: 0.008065302856266499\n",
            "Epoch 592, Loss: 0.007985658943653107\n",
            "Epoch 593, Loss: 0.00791426096111536\n",
            "Epoch 594, Loss: 0.007840955629944801\n",
            "Epoch 595, Loss: 0.007773494813591242\n",
            "Epoch 596, Loss: 0.007703997194766998\n",
            "Epoch 597, Loss: 0.007633656729012728\n",
            "Epoch 598, Loss: 0.0075711640529334545\n",
            "Epoch 599, Loss: 0.0075006806291639805\n",
            "Epoch 600, Loss: 0.007440999615937471\n",
            "Epoch 601, Loss: 0.00737367756664753\n",
            "Epoch 602, Loss: 0.007311628200113773\n",
            "Epoch 603, Loss: 0.007252932526171207\n",
            "Epoch 604, Loss: 0.007191787473857403\n",
            "Epoch 605, Loss: 0.007135253399610519\n",
            "Epoch 606, Loss: 0.007077759597450495\n",
            "Epoch 607, Loss: 0.007021040190011263\n",
            "Epoch 608, Loss: 0.006967214401811361\n",
            "Epoch 609, Loss: 0.006912303622812033\n",
            "Epoch 610, Loss: 0.006859032902866602\n",
            "Epoch 611, Loss: 0.006807336583733559\n",
            "Epoch 612, Loss: 0.006756230257451534\n",
            "Epoch 613, Loss: 0.00670646782964468\n",
            "Epoch 614, Loss: 0.006658536847680807\n",
            "Epoch 615, Loss: 0.006610579788684845\n",
            "Epoch 616, Loss: 0.006563203874975443\n",
            "Epoch 617, Loss: 0.006518160458654165\n",
            "Epoch 618, Loss: 0.006471811328083277\n",
            "Epoch 619, Loss: 0.006428457330912352\n",
            "Epoch 620, Loss: 0.0063838353380560875\n",
            "Epoch 621, Loss: 0.006341606378555298\n",
            "Epoch 622, Loss: 0.006299682892858982\n",
            "Epoch 623, Loss: 0.006258346140384674\n",
            "Epoch 624, Loss: 0.006219062488526106\n",
            "Epoch 625, Loss: 0.006179642863571644\n",
            "Epoch 626, Loss: 0.0061410400085151196\n",
            "Epoch 627, Loss: 0.006102832034230232\n",
            "Epoch 628, Loss: 0.006066211964935064\n",
            "Epoch 629, Loss: 0.0060290065594017506\n",
            "Epoch 630, Loss: 0.005993462633341551\n",
            "Epoch 631, Loss: 0.005958839785307646\n",
            "Epoch 632, Loss: 0.005925745237618685\n",
            "Epoch 633, Loss: 0.005894287023693323\n",
            "Epoch 634, Loss: 0.00585907232016325\n",
            "Epoch 635, Loss: 0.005826347041875124\n",
            "Epoch 636, Loss: 0.005795477423816919\n",
            "Epoch 637, Loss: 0.005762770306318998\n",
            "Epoch 638, Loss: 0.005733267404139042\n",
            "Epoch 639, Loss: 0.005702247843146324\n",
            "Epoch 640, Loss: 0.005672974046319723\n",
            "Epoch 641, Loss: 0.005643732845783234\n",
            "Epoch 642, Loss: 0.005615235771983862\n",
            "Epoch 643, Loss: 0.00558770727366209\n",
            "Epoch 644, Loss: 0.005561423487961292\n",
            "Epoch 645, Loss: 0.005534629803150892\n",
            "Epoch 646, Loss: 0.005510783288627863\n",
            "Epoch 647, Loss: 0.005486291833221912\n",
            "Epoch 648, Loss: 0.00545860081911087\n",
            "Epoch 649, Loss: 0.0054333750158548355\n",
            "Epoch 650, Loss: 0.0054150731302797794\n",
            "Epoch 651, Loss: 0.005389857105910778\n",
            "Epoch 652, Loss: 0.005374520551413298\n",
            "Epoch 653, Loss: 0.0053521269001066685\n",
            "Epoch 654, Loss: 0.005323313642293215\n",
            "Epoch 655, Loss: 0.005299820564687252\n",
            "Epoch 656, Loss: 0.005282770376652479\n",
            "Epoch 657, Loss: 0.00631432980298996\n",
            "Epoch 658, Loss: 0.006736452225595713\n",
            "Epoch 659, Loss: 0.015678007155656815\n",
            "Epoch 660, Loss: 0.01528188120573759\n",
            "Epoch 661, Loss: 0.04541356861591339\n",
            "Epoch 662, Loss: 0.01571880467236042\n",
            "Epoch 663, Loss: 0.06640984863042831\n",
            "Epoch 664, Loss: 0.11386234313249588\n",
            "Epoch 665, Loss: 0.10160162299871445\n",
            "Epoch 666, Loss: 0.08493036031723022\n",
            "Epoch 667, Loss: 0.09990974515676498\n",
            "Epoch 668, Loss: 0.06897314637899399\n",
            "Epoch 669, Loss: 0.0772826075553894\n",
            "Epoch 670, Loss: 0.04933663457632065\n",
            "Epoch 671, Loss: 0.049780603498220444\n",
            "Epoch 672, Loss: 0.02166096866130829\n",
            "Epoch 673, Loss: 0.015513520687818527\n",
            "Epoch 674, Loss: 0.01488554198294878\n",
            "Epoch 675, Loss: 0.01419022586196661\n",
            "Epoch 676, Loss: 0.014376946724951267\n",
            "Epoch 677, Loss: 0.01403186284005642\n",
            "Epoch 678, Loss: 0.012991703115403652\n",
            "Epoch 679, Loss: 0.0122152678668499\n",
            "Epoch 680, Loss: 0.011939405463635921\n",
            "Epoch 681, Loss: 0.0112986471503973\n",
            "Epoch 682, Loss: 0.010729815810918808\n",
            "Epoch 683, Loss: 0.01031388808041811\n",
            "Epoch 684, Loss: 0.010014432482421398\n",
            "Epoch 685, Loss: 0.009601419791579247\n",
            "Epoch 686, Loss: 0.00927557423710823\n",
            "Epoch 687, Loss: 0.009142845869064331\n",
            "Epoch 688, Loss: 0.008992782793939114\n",
            "Epoch 689, Loss: 0.008829363621771336\n",
            "Epoch 690, Loss: 0.008620855398476124\n",
            "Epoch 691, Loss: 0.008340965956449509\n",
            "Epoch 692, Loss: 0.008100722916424274\n",
            "Epoch 693, Loss: 0.007844110019505024\n",
            "Epoch 694, Loss: 0.007612103130668402\n",
            "Epoch 695, Loss: 0.007431303150951862\n",
            "Epoch 696, Loss: 0.007261390332132578\n",
            "Epoch 697, Loss: 0.007108189165592194\n",
            "Epoch 698, Loss: 0.006973112002015114\n",
            "Epoch 699, Loss: 0.006834832020103931\n",
            "Epoch 700, Loss: 0.006654061377048492\n",
            "Epoch 701, Loss: 0.006514033768326044\n",
            "Epoch 702, Loss: 0.006404244340956211\n",
            "Epoch 703, Loss: 0.006291186437010765\n",
            "Epoch 704, Loss: 0.006187465973198414\n",
            "Epoch 705, Loss: 0.006089826580137014\n",
            "Epoch 706, Loss: 0.00599259976297617\n",
            "Epoch 707, Loss: 0.005902810487896204\n",
            "Epoch 708, Loss: 0.005822818726301193\n",
            "Epoch 709, Loss: 0.005749888718128204\n",
            "Epoch 710, Loss: 0.005669637583196163\n",
            "Epoch 711, Loss: 0.005620948504656553\n",
            "Epoch 712, Loss: 0.005580160766839981\n",
            "Epoch 713, Loss: 0.005511846859008074\n",
            "Epoch 714, Loss: 0.0054471599869430065\n",
            "Epoch 715, Loss: 0.005407400894910097\n",
            "Epoch 716, Loss: 0.005380447022616863\n",
            "Epoch 717, Loss: 0.005327126011252403\n",
            "Epoch 718, Loss: 0.005282818805426359\n",
            "Epoch 719, Loss: 0.005239496938884258\n",
            "Epoch 720, Loss: 0.005193073768168688\n",
            "Epoch 721, Loss: 0.005156699102371931\n",
            "Epoch 722, Loss: 0.005125150550156832\n",
            "Epoch 723, Loss: 0.0050899614579975605\n",
            "Epoch 724, Loss: 0.005049873609095812\n",
            "Epoch 725, Loss: 0.005016354378312826\n",
            "Epoch 726, Loss: 0.004983269609510899\n",
            "Epoch 727, Loss: 0.004963153041899204\n",
            "Epoch 728, Loss: 0.004934337921440601\n",
            "Epoch 729, Loss: 0.004900583531707525\n",
            "Epoch 730, Loss: 0.004869602620601654\n",
            "Epoch 731, Loss: 0.004853575490415096\n",
            "Epoch 732, Loss: 0.0048302230425179005\n",
            "Epoch 733, Loss: 0.004803930874913931\n",
            "Epoch 734, Loss: 0.004778905306011438\n",
            "Epoch 735, Loss: 0.004761213902384043\n",
            "Epoch 736, Loss: 0.004745231941342354\n",
            "Epoch 737, Loss: 0.00472576729953289\n",
            "Epoch 738, Loss: 0.004704591352492571\n",
            "Epoch 739, Loss: 0.00469146016985178\n",
            "Epoch 740, Loss: 0.004675921518355608\n",
            "Epoch 741, Loss: 0.004660781007260084\n",
            "Epoch 742, Loss: 0.004645138513296843\n",
            "Epoch 743, Loss: 0.004630097653716803\n",
            "Epoch 744, Loss: 0.0046194992028176785\n",
            "Epoch 745, Loss: 0.004607591312378645\n",
            "Epoch 746, Loss: 0.004590651951730251\n",
            "Epoch 747, Loss: 0.004581116139888763\n",
            "Epoch 748, Loss: 0.004571208730340004\n",
            "Epoch 749, Loss: 0.0045583369210362434\n",
            "Epoch 750, Loss: 0.004547399003058672\n",
            "Epoch 751, Loss: 0.0045377942733466625\n",
            "Epoch 752, Loss: 0.0045278375037014484\n",
            "Epoch 753, Loss: 0.004519818350672722\n",
            "Epoch 754, Loss: 0.004508530721068382\n",
            "Epoch 755, Loss: 0.004502363968640566\n",
            "Epoch 756, Loss: 0.004492616280913353\n",
            "Epoch 757, Loss: 0.004485450219362974\n",
            "Epoch 758, Loss: 0.004479429684579372\n",
            "Epoch 759, Loss: 0.004467315971851349\n",
            "Epoch 760, Loss: 0.004464748315513134\n",
            "Epoch 761, Loss: 0.004458106588572264\n",
            "Epoch 762, Loss: 0.004446688573807478\n",
            "Epoch 763, Loss: 0.004443818237632513\n",
            "Epoch 764, Loss: 0.00443837558850646\n",
            "Epoch 765, Loss: 0.004431243985891342\n",
            "Epoch 766, Loss: 0.004424281883984804\n",
            "Epoch 767, Loss: 0.00442078011110425\n",
            "Epoch 768, Loss: 0.004415338858962059\n",
            "Epoch 769, Loss: 0.004408127162605524\n",
            "Epoch 770, Loss: 0.004405017010867596\n",
            "Epoch 771, Loss: 0.004399870987981558\n",
            "Epoch 772, Loss: 0.004395053256303072\n",
            "Epoch 773, Loss: 0.004390217363834381\n",
            "Epoch 774, Loss: 0.004385954700410366\n",
            "Epoch 775, Loss: 0.004382510203868151\n",
            "Epoch 776, Loss: 0.004376811441034079\n",
            "Epoch 777, Loss: 0.004373508971184492\n",
            "Epoch 778, Loss: 0.004369728732854128\n",
            "Epoch 779, Loss: 0.004365633707493544\n",
            "Epoch 780, Loss: 0.004361477214843035\n",
            "Epoch 781, Loss: 0.004358835518360138\n",
            "Epoch 782, Loss: 0.004354762379080057\n",
            "Epoch 783, Loss: 0.004351322539150715\n",
            "Epoch 784, Loss: 0.004349832888692617\n",
            "Epoch 785, Loss: 0.004346099216490984\n",
            "Epoch 786, Loss: 0.004345734603703022\n",
            "Epoch 787, Loss: 0.004345575347542763\n",
            "Epoch 788, Loss: 0.00434456579387188\n",
            "Epoch 789, Loss: 0.004338512197136879\n",
            "Epoch 790, Loss: 0.004332599695771933\n",
            "Epoch 791, Loss: 0.004337330348789692\n",
            "Epoch 792, Loss: 0.0043452465906739235\n",
            "Epoch 793, Loss: 0.004333776421844959\n",
            "Epoch 794, Loss: 0.004328731447458267\n",
            "Epoch 795, Loss: 0.004332812502980232\n",
            "Epoch 796, Loss: 0.00432336051017046\n",
            "Epoch 797, Loss: 0.004322465974837542\n",
            "Epoch 798, Loss: 0.004320499021559954\n",
            "Epoch 799, Loss: 0.0043135820887982845\n",
            "Epoch 800, Loss: 0.004313893616199493\n",
            "Epoch 801, Loss: 0.004312669858336449\n",
            "Epoch 802, Loss: 0.004304943606257439\n",
            "Epoch 803, Loss: 0.004304409027099609\n",
            "Epoch 804, Loss: 0.0043011917732656\n",
            "Epoch 805, Loss: 0.004299509339034557\n",
            "Epoch 806, Loss: 0.004299102816730738\n",
            "Epoch 807, Loss: 0.004295727238059044\n",
            "Epoch 808, Loss: 0.0042935023084282875\n",
            "Epoch 809, Loss: 0.004291050136089325\n",
            "Epoch 810, Loss: 0.004289864096790552\n",
            "Epoch 811, Loss: 0.00428794464096427\n",
            "Epoch 812, Loss: 0.004285621456801891\n",
            "Epoch 813, Loss: 0.00428282655775547\n",
            "Epoch 814, Loss: 0.0042820279486477375\n",
            "Epoch 815, Loss: 0.00427977042272687\n",
            "Epoch 816, Loss: 0.004279201850295067\n",
            "Epoch 817, Loss: 0.004277437459677458\n",
            "Epoch 818, Loss: 0.004275104030966759\n",
            "Epoch 819, Loss: 0.004274229519069195\n",
            "Epoch 820, Loss: 0.004274616949260235\n",
            "Epoch 821, Loss: 0.004273730795830488\n",
            "Epoch 822, Loss: 0.004269515164196491\n",
            "Epoch 823, Loss: 0.0042685712687671185\n",
            "Epoch 824, Loss: 0.004266773816198111\n",
            "Epoch 825, Loss: 0.004266385920345783\n",
            "Epoch 826, Loss: 0.004263099282979965\n",
            "Epoch 827, Loss: 0.004263421520590782\n",
            "Epoch 828, Loss: 0.004260907880961895\n",
            "Epoch 829, Loss: 0.004260099492967129\n",
            "Epoch 830, Loss: 0.004260305315256119\n",
            "Epoch 831, Loss: 0.004259119275957346\n",
            "Epoch 832, Loss: 0.004258158151060343\n",
            "Epoch 833, Loss: 0.004255922511219978\n",
            "Epoch 834, Loss: 0.004254618193954229\n",
            "Epoch 835, Loss: 0.004254082217812538\n",
            "Epoch 836, Loss: 0.004251749254763126\n",
            "Epoch 837, Loss: 0.004250446800142527\n",
            "Epoch 838, Loss: 0.004251760430634022\n",
            "Epoch 839, Loss: 0.004253225401043892\n",
            "Epoch 840, Loss: 0.004263131879270077\n",
            "Epoch 841, Loss: 0.004266176372766495\n",
            "Epoch 842, Loss: 0.004247413948178291\n",
            "Epoch 843, Loss: 0.004257899709045887\n",
            "Epoch 844, Loss: 0.0042455787770450115\n",
            "Epoch 845, Loss: 0.004250611178576946\n",
            "Epoch 846, Loss: 0.0042486293241381645\n",
            "Epoch 847, Loss: 0.00424128258600831\n",
            "Epoch 848, Loss: 0.004248094279319048\n",
            "Epoch 849, Loss: 0.004243527073413134\n",
            "Epoch 850, Loss: 0.0042400723323225975\n",
            "Epoch 851, Loss: 0.004245819989591837\n",
            "Epoch 852, Loss: 0.004241598770022392\n",
            "Epoch 853, Loss: 0.0042364769615232944\n",
            "Epoch 854, Loss: 0.004238185938447714\n",
            "Epoch 855, Loss: 0.004240042995661497\n",
            "Epoch 856, Loss: 0.004237323068082333\n",
            "Epoch 857, Loss: 0.004233916290104389\n",
            "Epoch 858, Loss: 0.004233264829963446\n",
            "Epoch 859, Loss: 0.004235735163092613\n",
            "Epoch 860, Loss: 0.004235734231770039\n",
            "Epoch 861, Loss: 0.004233716521412134\n",
            "Epoch 862, Loss: 0.004229839891195297\n",
            "Epoch 863, Loss: 0.004229080863296986\n",
            "Epoch 864, Loss: 0.004231094382703304\n",
            "Epoch 865, Loss: 0.004229888319969177\n",
            "Epoch 866, Loss: 0.004229193087667227\n",
            "Epoch 867, Loss: 0.004228014498949051\n",
            "Epoch 868, Loss: 0.004226557444781065\n",
            "Epoch 869, Loss: 0.004226334858685732\n",
            "Epoch 870, Loss: 0.00422623660415411\n",
            "Epoch 871, Loss: 0.004226239398121834\n",
            "Epoch 872, Loss: 0.004225655924528837\n",
            "Epoch 873, Loss: 0.004224301781505346\n",
            "Epoch 874, Loss: 0.0042241355404257774\n",
            "Epoch 875, Loss: 0.004223908297717571\n",
            "Epoch 876, Loss: 0.0042245760560035706\n",
            "Epoch 877, Loss: 0.00422442052513361\n",
            "Epoch 878, Loss: 0.0042223273776471615\n",
            "Epoch 879, Loss: 0.004221756011247635\n",
            "Epoch 880, Loss: 0.004220498260110617\n",
            "Epoch 881, Loss: 0.004221780691295862\n",
            "Epoch 882, Loss: 0.004221140407025814\n",
            "Epoch 883, Loss: 0.00422078650444746\n",
            "Epoch 884, Loss: 0.00421902583912015\n",
            "Epoch 885, Loss: 0.004218350630253553\n",
            "Epoch 886, Loss: 0.00421868497505784\n",
            "Epoch 887, Loss: 0.004217125475406647\n",
            "Epoch 888, Loss: 0.004217785783112049\n",
            "Epoch 889, Loss: 0.004217357374727726\n",
            "Epoch 890, Loss: 0.004216543864458799\n",
            "Epoch 891, Loss: 0.004215169232338667\n",
            "Epoch 892, Loss: 0.0042155650444328785\n",
            "Epoch 893, Loss: 0.004213894251734018\n",
            "Epoch 894, Loss: 0.004214718472212553\n",
            "Epoch 895, Loss: 0.004214304964989424\n",
            "Epoch 896, Loss: 0.0042136902920901775\n",
            "Epoch 897, Loss: 0.004213161766529083\n",
            "Epoch 898, Loss: 0.004212157800793648\n",
            "Epoch 899, Loss: 0.004212742205709219\n",
            "Epoch 900, Loss: 0.004212059546262026\n",
            "Epoch 901, Loss: 0.004212474916130304\n",
            "Epoch 902, Loss: 0.004211509134620428\n",
            "Epoch 903, Loss: 0.0042114220559597015\n",
            "Epoch 904, Loss: 0.004211879801005125\n",
            "Epoch 905, Loss: 0.004212528932839632\n",
            "Epoch 906, Loss: 0.004213140346109867\n",
            "Epoch 907, Loss: 0.004213178995996714\n",
            "Epoch 908, Loss: 0.004211875144392252\n",
            "Epoch 909, Loss: 0.004210356157273054\n",
            "Epoch 910, Loss: 0.004211458843201399\n",
            "Epoch 911, Loss: 0.004211048595607281\n",
            "Epoch 912, Loss: 0.004209620412439108\n",
            "Epoch 913, Loss: 0.004210858605802059\n",
            "Epoch 914, Loss: 0.004211426712572575\n",
            "Epoch 915, Loss: 0.004210973158478737\n",
            "Epoch 916, Loss: 0.004210431128740311\n",
            "Epoch 917, Loss: 0.004208239261060953\n",
            "Epoch 918, Loss: 0.004209217149764299\n",
            "Epoch 919, Loss: 0.004209069535136223\n",
            "Epoch 920, Loss: 0.004208283964544535\n",
            "Epoch 921, Loss: 0.004207276273518801\n",
            "Epoch 922, Loss: 0.004207689315080643\n",
            "Epoch 923, Loss: 0.004206682555377483\n",
            "Epoch 924, Loss: 0.004207251593470573\n",
            "Epoch 925, Loss: 0.004206230863928795\n",
            "Epoch 926, Loss: 0.0042061335407197475\n",
            "Epoch 927, Loss: 0.004206381738185883\n",
            "Epoch 928, Loss: 0.004205435048788786\n",
            "Epoch 929, Loss: 0.004206118639558554\n",
            "Epoch 930, Loss: 0.00420480128377676\n",
            "Epoch 931, Loss: 0.004205329809337854\n",
            "Epoch 932, Loss: 0.004204931668937206\n",
            "Epoch 933, Loss: 0.0042044250294566154\n",
            "Epoch 934, Loss: 0.0042047989554703236\n",
            "Epoch 935, Loss: 0.004203977063298225\n",
            "Epoch 936, Loss: 0.0042044329456985\n",
            "Epoch 937, Loss: 0.0042037153616547585\n",
            "Epoch 938, Loss: 0.004203967750072479\n",
            "Epoch 939, Loss: 0.004203506745398045\n",
            "Epoch 940, Loss: 0.004203399643301964\n",
            "Epoch 941, Loss: 0.004203558899462223\n",
            "Epoch 942, Loss: 0.004202916752547026\n",
            "Epoch 943, Loss: 0.0042031388729810715\n",
            "Epoch 944, Loss: 0.0042027258314192295\n",
            "Epoch 945, Loss: 0.0042024836875498295\n",
            "Epoch 946, Loss: 0.004202589392662048\n",
            "Epoch 947, Loss: 0.004202131647616625\n",
            "Epoch 948, Loss: 0.004202382173389196\n",
            "Epoch 949, Loss: 0.0042022159323096275\n",
            "Epoch 950, Loss: 0.004202188458293676\n",
            "Epoch 951, Loss: 0.004202273208647966\n",
            "Epoch 952, Loss: 0.004202236887067556\n",
            "Epoch 953, Loss: 0.004202838987112045\n",
            "Epoch 954, Loss: 0.0042045540176332\n",
            "Epoch 955, Loss: 0.004205348435789347\n",
            "Epoch 956, Loss: 0.004203473217785358\n",
            "Epoch 957, Loss: 0.004201679490506649\n",
            "Epoch 958, Loss: 0.004203552845865488\n",
            "Epoch 959, Loss: 0.004202768672257662\n",
            "Epoch 960, Loss: 0.004201647359877825\n",
            "Epoch 961, Loss: 0.004202334675937891\n",
            "Epoch 962, Loss: 0.004201535135507584\n",
            "Epoch 963, Loss: 0.004201740492135286\n",
            "Epoch 964, Loss: 0.0042011975310742855\n",
            "Epoch 965, Loss: 0.0042014941573143005\n",
            "Epoch 966, Loss: 0.004200686700642109\n",
            "Epoch 967, Loss: 0.004201255738735199\n",
            "Epoch 968, Loss: 0.004200572147965431\n",
            "Epoch 969, Loss: 0.0042005558498203754\n",
            "Epoch 970, Loss: 0.004200706258416176\n",
            "Epoch 971, Loss: 0.004200120456516743\n",
            "Epoch 972, Loss: 0.0042001777328550816\n",
            "Epoch 973, Loss: 0.004200004041194916\n",
            "Epoch 974, Loss: 0.004199776332825422\n",
            "Epoch 975, Loss: 0.004200024995952845\n",
            "Epoch 976, Loss: 0.004199528601020575\n",
            "Epoch 977, Loss: 0.004199918359518051\n",
            "Epoch 978, Loss: 0.004199627786874771\n",
            "Epoch 979, Loss: 0.004200237803161144\n",
            "Epoch 980, Loss: 0.004201049450784922\n",
            "Epoch 981, Loss: 0.004201181698590517\n",
            "Epoch 982, Loss: 0.004200705327093601\n",
            "Epoch 983, Loss: 0.004201204981654882\n",
            "Epoch 984, Loss: 0.00420223968103528\n",
            "Epoch 985, Loss: 0.004205870907753706\n",
            "Epoch 986, Loss: 0.004272157792001963\n",
            "Epoch 987, Loss: 0.004642970860004425\n",
            "Epoch 988, Loss: 0.010062074288725853\n",
            "Epoch 989, Loss: 0.012241797521710396\n",
            "Epoch 990, Loss: 0.012063405476510525\n",
            "Epoch 991, Loss: 0.010886049829423428\n",
            "Epoch 992, Loss: 0.008265238255262375\n",
            "Epoch 993, Loss: 0.02563941292464733\n",
            "Epoch 994, Loss: 0.04034874215722084\n",
            "Epoch 995, Loss: 0.028599247336387634\n",
            "Epoch 996, Loss: 0.061403412371873856\n",
            "Epoch 997, Loss: 0.06657935678958893\n",
            "Epoch 998, Loss: 0.06661336869001389\n",
            "Epoch 999, Loss: 0.051750894635915756\n",
            "Epoch 1000, Loss: 0.037463460117578506\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def infer(model, tokenizer, input_text, max_length=250, device='cuda'):\n",
        "    # Tokenize the input text\n",
        "    encoded = tokenizer.encode(input_text)\n",
        "    input_ids = torch.tensor(encoded.ids, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    # Generate text\n",
        "    generated_ids = input_ids\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            # Pass the input through the model\n",
        "            outputs = model(generated_ids)\n",
        "            # Get the last token logits\n",
        "            next_token_logits = outputs[:, -1, :]\n",
        "            # Sample the next token\n",
        "            next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)\n",
        "            # Append the next token to the sequence\n",
        "            generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)\n",
        "            # Stop if the EOS token is generated\n",
        "            if next_token_id.item() == tokenizer.token_to_id(\"[EOS]\"):\n",
        "                break\n",
        "\n",
        "    # Decode the generated sequence\n",
        "    decoded_text = tokenizer.decode(generated_ids.squeeze().tolist(), skip_special_tokens=True)\n",
        "    return decoded_text\n",
        "\n",
        "# Example usage\n",
        "input_text = \"Lorem ipsum\"\n",
        "generated_text = infer(model, tokenizer, input_text, max_length=50)\n",
        "print(f\"Generated text: {generated_text}\")"
      ],
      "metadata": {
        "id": "clhTPJMW69sP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8466041-d53a-4ad6-d4f4-0acc5b1349ba"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text: Lorem ipsum ac fames dictumst Diam blandit ut cubilia ullamcorper vel laoreet rhoncus . primis facilisis . Sollicitudin tempor elementum placerat viverra ; porta vivamus nibh ante natoque Potenti tincidunt tincidunt porta nascetur est fames malesuada ad . rhoncus quam montes ante . rhoncus ex orci . Pretium orci ; auctor cursus\n"
          ]
        }
      ]
    }
  ]
}